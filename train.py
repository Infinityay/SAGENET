import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
import logging
from tqdm import tqdm
import os
from pathlib import Path
from datetime import datetime
import random
import torch.nn.functional as F
from models import create_model, get_model_name
from preparation import NetworkPacketPreprocessor, create_data_loaders
from config import (
    DATA_CONFIG, TRAINING_CONFIG, PATH_CONFIG,
    COLFORMER_DYNAMIC_SGP_CONFIG  
)


def set_seed(seed=42):
    """è®¾ç½®å…¨å±€éšæœºç§å­ä»¥ç¡®ä¿å¯é‡ç°æ€§"""
    # Pythonéšæœºæ•°
    random.seed(seed)
    
    # NumPyéšæœºæ•°
    np.random.seed(seed)
    
    # PyTorchéšæœºæ•°
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    
    # ç¡®ä¿CUDAæ“ä½œçš„ç¡®å®šæ€§
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    # ä½¿ç”¨ç¡®å®šæ€§ç®—æ³• - ä½†å¯¹ä¸æ”¯æŒçš„æ“ä½œåªå‘å‡ºè­¦å‘Š
    torch.use_deterministic_algorithms(True, warn_only=True)
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['PYTHONHASHSEED'] = str(seed)
    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'
    
    logger.info(f"éšæœºç§å­å·²è®¾ç½®ä¸º: {seed}")

# è®¾ç½®æ—¥å¿— - åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶
def setup_logging():
    # åˆ›å»ºæŒ‰æ—¥æœŸåˆ†ç±»çš„logsç›®å½•
    current_date = datetime.now().strftime("%Y-%m-%d")
    log_dir = Path(PATH_CONFIG['log_dir']) / current_date
    log_dir.mkdir(parents=True, exist_ok=True)
    
    # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ—¥å¿—æ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = log_dir / f"training_{timestamp}.log"
    
    # é…ç½®æ—¥å¿—æ ¼å¼
    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    
    # æ¸…é™¤ç°æœ‰çš„å¤„ç†å™¨
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    
    # é…ç½®æ—¥å¿—è®°å½•å™¨
    logging.basicConfig(
        level=logging.INFO,
        format=log_format,
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),  # æ–‡ä»¶å¤„ç†å™¨
            logging.StreamHandler()  # æ§åˆ¶å°å¤„ç†å™¨
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"æ—¥å¿—å°†ä¿å­˜åˆ°: {log_file}")
    return logger

logger = setup_logging()


class ProtocolTrainer:
    def __init__(self, model, train_loader, val_loader, test_loader, device):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.test_loader = test_loader
        self.device = device
        
        # æ—©åœé…ç½® - ä¿®æ”¹ä¸ºåŸºäºF1æ”¹è¿›çš„æ—©åœ
        self.early_stop_patience = 3       # è¿ç»­æ²¡æœ‰æ”¹è¿›çš„epochæ•°
        self.early_stop_counter = 0        # è®¡æ•°å™¨

        
        # ä¼˜åŒ–å™¨ - ä½¿ç”¨æ›´ç¨³å®šçš„å­¦ä¹ ç‡
        self.optimizer = optim.AdamW(
            model.parameters(), 
            lr=TRAINING_CONFIG['learning_rate'], 
            weight_decay=TRAINING_CONFIG['weight_decay']
        )
        

        self.criterion = nn.CrossEntropyLoss(ignore_index=-100)
        logger.info("ä½¿ç”¨æ ‡å‡†äº¤å‰ç†µæŸå¤±")

        self.scheduler = None
        logger.info("ä½¿ç”¨å›ºå®šå­¦ä¹ ç‡è®­ç»ƒ")

        # è®°å½•æœ€ä½³æ¨¡å‹ï¼ˆä»…ç”¨äºæ—¥å¿—æ˜¾ç¤ºï¼‰
        self.best_val_f1 = 0.0
        self.best_val_loss = float('inf')
        self.best_epoch = 0
        self.best_model_path = PATH_CONFIG['best_model_path']
        
        # è®­ç»ƒå†å²è®°å½•
        self.train_history = {
            'train_loss': [], 'train_f1': [],
            'val_loss': [], 'val_precision': [], 'val_recall': [], 'val_f1': [],
            'learning_rates': []
        }
        
    def train_epoch(self):
        """è®­ç»ƒä¸€ä¸ªepoch - ä½¿ç”¨æ”¹è¿›çš„åˆ†ç»„å…±è¯†è¯„ä¼°"""
        self.model.train()
        total_loss = 0
        protocol_results = {}
        
        progress_bar = tqdm(self.train_loader, desc="Training")
        
        for batch_idx, batch in enumerate(progress_bar):
            self.optimizer.zero_grad()
            
            bit_matrices = batch['bit_matrix'].to(self.device)
            labels = batch['labels'].to(self.device)
            protocols = batch['protocols']
            
            attention_mask = (bit_matrices.sum(dim=-1) > 0)
            
            # è·å–ç»“æ„ç‰¹å¾
            structure_features = batch.get('structure_features')
            if structure_features is not None:
                structure_features = structure_features.to(self.device)
            
            # åªè°ƒç”¨ ColFormerDynamicSGP æ¨¡å‹
            logits = self.model(
                bit_matrix=bit_matrices,
                structure_features=structure_features,
                attention_mask=attention_mask
            )
                 
            
            # åªè®¡ç®—äº¤å‰ç†µæŸå¤±
            ce_loss = self.criterion(logits.view(-1, 2), labels.view(-1))
            
            # åå‘ä¼ æ’­
            ce_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            
            # ç´¯è®¡æŸå¤±
            total_loss += ce_loss.item()
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                'loss': ce_loss.item()
            })
            
            # æ”¶é›†é¢„æµ‹ç»“æœç”¨äºè®¡ç®—å…±è¯†æŒ‡æ ‡
            with torch.no_grad():
                preds = torch.argmax(logits, dim=2)
                mask = labels != -100
                
                for i in range(len(protocols)):
                    protocol = protocols[i]
                    sample_mask = mask[i]
                    
                    if sample_mask.any():
                        sample_preds = preds[i][sample_mask].cpu().numpy()
                        sample_labels = labels[i][sample_mask].cpu().numpy()

                        if protocol not in protocol_results:
                            protocol_results[protocol] = {'preds': [], 'labels': []}
                        
                        protocol_results[protocol]['preds'].append(sample_preds.tolist())
                        protocol_results[protocol]['labels'].append(sample_labels.tolist())
        
        # è®¡ç®—å¹³å‡æŸå¤±
        avg_loss = total_loss / len(self.train_loader)
        
        # ä½¿ç”¨æ”¹è¿›çš„åˆ†ç»„å…±è¯†è¯„ä¼°è®¡ç®—è®­ç»ƒF1
        protocol_f1_scores = []
        for protocol, data in protocol_results.items():
            if data['labels'] and len(data['preds']) >= 1:
                metrics = self.evaluate_consensus_prediction(
                    data['preds'], data['labels'], threshold_ratio=0.5
                )
                protocol_f1_scores.append(metrics['f1'])
        
        train_f1 = np.mean(protocol_f1_scores) if protocol_f1_scores else 0.0
        
        # è®°å½•æŸå¤±
        logger.info(f"è®­ç»ƒæŸå¤±: {avg_loss:.4f}")
        
        return avg_loss, train_f1
    
    def validate(self, data_loader, desc="Validation"):
        """éªŒè¯æ¨¡å‹ -  ä½¿ç”¨æ”¹è¿›çš„åˆ†ç»„å…±è¯†è¯„ä¼°"""
        self.model.eval()
        total_loss = 0
        protocol_results = {}

        with torch.no_grad():
            for batch in tqdm(data_loader, desc=desc):
                bit_matrices = batch['bit_matrix'].to(self.device)
                labels = batch['labels'].to(self.device)
                protocols = batch['protocols']
                
                attention_mask = (bit_matrices.sum(dim=-1) > 0)
            
                # è·å–ç»“æ„ç‰¹å¾
                structure_features = batch.get('structure_features')
                if structure_features is not None:
                    structure_features = structure_features.to(self.device)
                
                # åªè°ƒç”¨ ColFormerDynamicSGP æ¨¡å‹
                logits = self.model(
                    bit_matrix=bit_matrices,
                    structure_features=structure_features,
                    attention_mask=attention_mask
                )

                loss = self.criterion(logits.view(-1, 2), labels.view(-1))
                total_loss += loss.item()

                preds = torch.argmax(logits, dim=2)
                mask = labels != -100

                # æ”¶é›†é¢„æµ‹ç»“æœ
                for i in range(len(protocols)):
                    protocol = protocols[i]
                    sample_mask = mask[i]
                    
                    if sample_mask.any():
                        sample_preds = preds[i][sample_mask].cpu().numpy()
                        sample_labels = labels[i][sample_mask].cpu().numpy()

                        if protocol not in protocol_results:
                            protocol_results[protocol] = {'preds': [], 'labels': []}
                        
                        protocol_results[protocol]['preds'].append(sample_preds.tolist())
                        protocol_results[protocol]['labels'].append(sample_labels.tolist())

        avg_loss = total_loss / len(data_loader)

        # è®¡ç®—æ¯ä¸ªåè®®çš„åˆ†ç»„åˆ†ç»„å…±è¯†æŒ‡æ ‡
        protocol_metrics = {}
        
        for protocol, data in protocol_results.items():
            if data['labels'] and len(data['preds']) >= 1:
                # ä½¿ç”¨æ”¹è¿›çš„åˆ†ç»„æ”¹è¿›çš„åˆ†ç»„å…±è¯†è¯„ä¼°
                metrics = self.evaluate_consensus_prediction(
                    data['preds'], data['labels'], threshold_ratio=0.5
                )
                protocol_metrics[protocol] = metrics
                
                
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡ - ä»å„åè®®æŒ‡æ ‡è®¡ç®—å¹³å‡å€¼
        if protocol_metrics:
            overall_metrics = {
                'precision': np.mean([m['precision'] for m in protocol_metrics.values()]),
                'recall': np.mean([m['recall'] for m in protocol_metrics.values()]),
                'f1': np.mean([m['f1'] for m in protocol_metrics.values()]),
                'tp': sum([m['tp'] for m in protocol_metrics.values()]),
                'fp': sum([m['fp'] for m in protocol_metrics.values()]),
                'fn': sum([m['fn'] for m in protocol_metrics.values()])
            }
        else:
            overall_metrics = {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'tp': 0, 'fp': 0, 'fn': 0}

        return {
            'loss': avg_loss,
            'overall': overall_metrics,
            'by_protocol': protocol_metrics
        }

    
    def calculate_byte_level_metrics(self, all_preds, all_labels):
        """
        å±‚æ¬¡ä¸€ï¼šå­—èŠ‚çº§è¾¹ç•Œè¯„ä¼° (Byte-level Boundary Evaluation)

        è®¡ç®—å­—èŠ‚çº§è¾¹ç•Œè¯„ä¼°çš„æ ¸å¿ƒæŒ‡æ ‡ï¼š
        - ç²¾ç¡®ç‡ (Precision): TP / (TP + FP)
        - å¬å›ç‡ (Recall): TP / (TP + FN)
        - F1åˆ†æ•° (F1-Score): 2 * (Precision * Recall) / (Precision + Recall)
        """
        if len(all_preds) == 0 or len(all_labels) == 0:
            return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'tp': 0, 'fp': 0, 'fn': 0}

        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        preds = np.array(all_preds)
        labels = np.array(all_labels)

        # ç¬¬2æ­¥ï¼šé€ç‚¹æ¯”è¾ƒä¸è®¡æ•°
        # éå†æ¯ä¸€ä¸ªä½ç½®ï¼Œç´¯åŠ ä¸‰ç§æƒ…å†µçš„è®¡æ•°
        tp = np.sum((preds == 1) & (labels == 1))  # çœŸé˜³æ€§ï¼šæ¨¡å‹æ­£ç¡®æ‰¾åˆ°è¾¹ç•Œ
        fp = np.sum((preds == 1) & (labels == 0))  # å‡é˜³æ€§ï¼šæ¨¡å‹é”™è¯¯åˆ›é€ è¾¹ç•Œ
        fn = np.sum((preds == 0) & (labels == 1))  # å‡é˜´æ€§ï¼šæ¨¡å‹é—æ¼è¾¹ç•Œ

        # ç¬¬3æ­¥ï¼šè®¡ç®—æ ¸å¿ƒæŒ‡æ ‡
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0

        return {
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'tp': int(tp),
            'fp': int(fp),
            'fn': int(fn)
        }
    
    def evaluate_consensus_prediction(self, predictions: list, ground_truths: list, threshold_ratio=0.5):
        """æ”¹è¿›çš„å…±è¯†è¯„ä¼°ï¼šæŒ‰é•¿åº¦åˆ†ç»„åˆ†åˆ«è®¡ç®—å…±è¯†ï¼Œç„¶ååŠ æƒå¹³å‡"""
        if len(predictions) < 2:
            return self.calculate_byte_level_metrics(
                [item for sublist in predictions for item in sublist],
                [item for sublist in ground_truths for item in sublist]
            )

        # Step 1: æŒ‰é•¿åº¦åˆ†ç»„
        length_groups = {}
        for pred, gt in zip(predictions, ground_truths):
            length = len(pred)
            if length not in length_groups:
                length_groups[length] = {'preds': [], 'gts': [], 'count': 0}
            length_groups[length]['preds'].append(pred)
            length_groups[length]['gts'].append(gt)
            length_groups[length]['count'] += 1
        
        logger.debug(f"é•¿åº¦åˆ†å¸ƒ: {[(length, group['count']) for length, group in length_groups.items()]}")
        
        # Step 2: å¯¹æ¯ä¸ªé•¿åº¦ç»„åˆ†åˆ«è®¡ç®—å…±è¯†
        group_metrics = {}
        total_samples = sum(group['count'] for group in length_groups.values())
        
        for length, group in length_groups.items():
            if group['count'] >= 2:  # è‡³å°‘éœ€è¦2ä¸ªæ ·æœ¬æ‰èƒ½è®¡ç®—å…±è¯†
                # è®¡ç®—è¯¥é•¿åº¦ç»„çš„å…±è¯†
                consensus_pred = self._calculate_consensus_sequence(group['preds'], threshold_ratio)
                consensus_gt = self._calculate_consensus_sequence(group['gts'], 0.5)
                
                metrics = self.calculate_byte_level_metrics(consensus_pred, consensus_gt)
                metrics['sample_count'] = group['count']
                metrics['weight'] = group['count'] / total_samples
                group_metrics[length] = metrics
                
                logger.debug(f"é•¿åº¦{length}: {group['count']}ä¸ªæ ·æœ¬, F1={metrics['f1']:.4f}, æƒé‡={metrics['weight']:.3f}")
        
        # Step 3: åŠ æƒå¹³å‡è®¡ç®—æ€»ä½“æŒ‡æ ‡
        if not group_metrics:
            logger.warning("æ²¡æœ‰è¶³å¤Ÿæ ·æœ¬çš„é•¿åº¦ç»„è¿›è¡Œå…±è¯†è¯„ä¼°")
            return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'tp': 0, 'fp': 0, 'fn': 0}
        
        weighted_precision = sum(m['precision'] * m['weight'] for m in group_metrics.values())
        weighted_recall = sum(m['recall'] * m['weight'] for m in group_metrics.values())
        weighted_f1 = sum(m['f1'] * m['weight'] for m in group_metrics.values())
        
        total_tp = sum(m['tp'] for m in group_metrics.values())
        total_fp = sum(m['fp'] for m in group_metrics.values())
        total_fn = sum(m['fn'] for m in group_metrics.values())
        
        # è®°å½•è¯¦ç»†ä¿¡æ¯
        length_info = [f"{length}å­—èŠ‚:{group['count']}ä¸ª" for length, group in length_groups.items()]
        logger.debug(f"åˆ†ç»„å…±è¯†è¯„ä¼°å®Œæˆ - {', '.join(length_info)}")
        
        return {
            'precision': weighted_precision,
            'recall': weighted_recall,
            'f1': weighted_f1,
            'tp': total_tp,
            'fp': total_fp,
            'fn': total_fn,
            'length_groups': group_metrics,
            'total_samples': total_samples
        }
    
    
    def _calculate_consensus_sequence(self, sequences, threshold_ratio):
        """è®¡ç®—åºåˆ—çš„å…±è¯†"""
        if not sequences:
            return []
        
        length = len(sequences[0])
        num_sequences = len(sequences)
        consensus = []
        
        for pos in range(length):
            votes = sum(seq[pos] for seq in sequences)
            consensus.append(1 if votes >= (num_sequences * threshold_ratio) else 0)
        
        return consensus
    
    def save_best_model(self, epoch, val_f1, val_loss, val_precision, val_recall):
        """ä¿å­˜æœ€ä½³æ¨¡å‹ï¼šF1æœ€é«˜ï¼ŒF1ç›¸åŒæ—¶é€‰æ‹©val lossæœ€ä½çš„"""
        is_best = False
        
        if val_f1 > self.best_val_f1:
            # F1æ›´é«˜ï¼Œç›´æ¥ä¿å­˜
            is_best = True
            logger.info(f"ğŸ‰ æ–°çš„æœ€ä½³F1: {val_f1:.4f} (ä¸Šä¸€æ¬¡: {self.best_val_f1:.4f})")
        elif val_f1 == self.best_val_f1 and val_loss < self.best_val_loss:
            # F1ç›¸åŒä½†éªŒè¯æŸå¤±æ›´ä½ï¼Œä¹Ÿä¿å­˜
            is_best = True
            logger.info(f"ğŸ‰ ç›¸åŒF1ä½†æ›´ä½éªŒè¯æŸå¤±: {val_loss:.4f} (ä¸Šä¸€æ¬¡: {self.best_val_loss:.4f})")
        
        if is_best:
            self.best_val_f1 = val_f1
            self.best_val_loss = val_loss
            self.best_epoch = epoch
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            model_state_dict = (self.model.module.state_dict() 
                              if isinstance(self.model, nn.DataParallel) 
                              else self.model.state_dict())
            
            checkpoint_data = {
                'epoch': epoch,
                'model_state_dict': model_state_dict,
                'optimizer_state_dict': self.optimizer.state_dict(),
                'val_f1': float(val_f1),
                'val_precision': float(val_precision),
                'val_recall': float(val_recall),
                'val_loss': float(val_loss),
                'train_history': self.train_history,
                'is_best_model': True
            }
            
            # åªæœ‰å½“schedulerå­˜åœ¨æ—¶æ‰ä¿å­˜å…¶çŠ¶æ€
            if self.scheduler is not None:
                checkpoint_data['scheduler_state_dict'] = self.scheduler.state_dict()
            
            torch.save(checkpoint_data, self.best_model_path)
            
            logger.info(f"ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ°: {self.best_model_path}")
        
        return is_best

    def train(self, num_epochs=5):
        """å®Œæ•´è®­ç»ƒæµç¨‹ - ä¿®æ”¹æ—©åœé€»è¾‘ä¸ºåŸºäºF1æ”¹è¿›"""
        logger.info(f"å¼€å§‹è®­ç»ƒï¼Œå…± {num_epochs} ä¸ªepoch")

        for epoch in range(num_epochs):
            logger.info(f"\n=== Epoch {epoch+1}/{num_epochs} ===")
            
            # è®­ç»ƒ
            train_loss, train_f1 = self.train_epoch()
            
            # éªŒè¯
            val_results = self.validate(self.val_loader, "Validation")
            
            # è®°å½•å†å²
            overall = val_results['overall']
            current_lr = self.optimizer.param_groups[0]['lr']
            self.train_history['train_loss'].append(train_loss)
            self.train_history['train_f1'].append(train_f1)
            self.train_history['val_loss'].append(val_results['loss'])
            self.train_history['val_f1'].append(overall['f1'])
            self.train_history['val_precision'].append(overall['precision'])
            self.train_history['val_recall'].append(overall['recall'])
            self.train_history['learning_rates'].append(current_lr)
            
            # è®°å½•ç»“æœ
            logger.info(f"Train - Loss: {train_loss:.4f}, F1: {train_f1:.4f}")
            logger.info(f"Val Overall - Loss: {val_results['loss']:.4f}, F1: {overall['f1']:.4f}, "
                       f"Precision: {overall['precision']:.4f}, Recall: {overall['recall']:.4f}")
            logger.info(f"Learning Rate: {current_lr:.2e}")
            
            # æ˜¾ç¤ºå„åè®®æŒ‡æ ‡
            logger.info("å„åè®®éªŒè¯æŒ‡æ ‡:")
            for protocol, metrics in val_results['by_protocol'].items():
                logger.info(f"  {protocol:>8} - F1: {metrics['f1']:.4f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹å¹¶æ£€æŸ¥æ˜¯å¦æœ‰æ”¹è¿›
            is_best = self.save_best_model(
                epoch, overall['f1'], val_results['loss'], 
                overall['precision'], overall['recall']
            )
            
            # æ—©åœæ£€æŸ¥ - åŸºäºF1æ”¹è¿›
            if is_best:
                # æœ‰æ”¹è¿›ï¼Œé‡ç½®è®¡æ•°å™¨
                self.early_stop_counter = 0
                logger.info(f"âœ… æ¨¡å‹æœ‰æ”¹è¿›ï¼Œé‡ç½®æ—©åœè®¡æ•°å™¨")
            else:
                # æ— æ”¹è¿›ï¼Œå¢åŠ è®¡æ•°å™¨
                self.early_stop_counter += 1
                logger.info(f"âš ï¸  è¿ç»­{self.early_stop_counter}ä¸ªepochæ— æ”¹è¿› "
                           f"(å½“å‰F1: {overall['f1']:.4f}, æœ€ä½³F1: {self.best_val_f1:.4f})")
                
                if self.early_stop_counter >= self.early_stop_patience:
                    logger.info(f"ğŸ›‘ è¿ç»­{self.early_stop_patience}ä¸ªepochæ— æ”¹è¿›ï¼Œæå‰åœæ­¢è®­ç»ƒ")
                    break
        
        logger.info(f"\n=== è®­ç»ƒå®Œæˆ ===")
        if self.early_stop_counter >= self.early_stop_patience:
            logger.info(f"æå‰åœæ­¢äº Epoch {len(self.train_history['train_loss'])} (æ— æ”¹è¿›)")
        else:
            logger.info(f"æ­£å¸¸å®Œæˆè®­ç»ƒ (Epoch {len(self.train_history['train_loss'])})")
        
        logger.info(f"æœ€ä½³æ¨¡å‹æ¥è‡ª Epoch {self.best_epoch + 1}")
        logger.info(f"æœ€ä½³éªŒè¯F1: {self.best_val_f1:.4f}")
        logger.info(f"æœ€ä½³éªŒè¯Loss: {self.best_val_loss:.4f}")
        logger.info(f"æ€»è®­ç»ƒè½®æ•°: {len(self.train_history['train_loss'])}")
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self.plot_training_curves()

    def plot_training_curves(self):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        try:
            import matplotlib.pyplot as plt
            
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            epochs = range(1, len(self.train_history['train_loss']) + 1)
            
            # æŸå¤±æ›²çº¿
            axes[0, 0].plot(epochs, self.train_history['train_loss'], 'b-', label='Train Loss')
            axes[0, 0].plot(epochs, self.train_history['val_loss'], 'r-', label='Val Loss')
            axes[0, 0].set_title('Training and Validation Loss')
            axes[0, 0].set_xlabel('Epoch')
            axes[0, 0].set_ylabel('Loss')
            axes[0, 0].legend()
            axes[0, 0].grid(True)
            
            # F1åˆ†æ•°æ›²çº¿
            axes[0, 1].plot(epochs, self.train_history['train_f1'], 'b-', label='Train F1')
            axes[0, 1].plot(epochs, self.train_history['val_f1'], 'r-', label='Val F1')
            axes[0, 1].set_title('Training and Validation F1 Score')
            axes[0, 1].set_xlabel('Epoch')
            axes[0, 1].set_ylabel('F1 Score')
            axes[0, 1].legend()
            axes[0, 1].grid(True)
            
            # ç²¾ç¡®ç‡å’Œå¬å›ç‡
            axes[1, 0].plot(epochs, self.train_history['val_precision'], 'g-', label='Precision')
            axes[1, 0].plot(epochs, self.train_history['val_recall'], 'orange', label='Recall')
            axes[1, 0].plot(epochs, self.train_history['val_f1'], 'r-', label='F1')
            axes[1, 0].set_title('Validation Metrics')
            axes[1, 0].set_xlabel('Epoch')
            axes[1, 0].set_ylabel('Score')
            axes[1, 0].legend()
            axes[1, 0].grid(True)
            
            # å­¦ä¹ ç‡æ›²çº¿
            axes[1, 1].plot(epochs, self.train_history['learning_rates'], 'purple', label='Learning Rate')
            axes[1, 1].set_title('Learning Rate Schedule')
            axes[1, 1].set_xlabel('Epoch')
            axes[1, 1].set_ylabel('Learning Rate')
            axes[1, 1].set_yscale('log')
            axes[1, 1].legend()
            axes[1, 1].grid(True)
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾ç‰‡
            plots_dir = Path("plots")
            plots_dir.mkdir(exist_ok=True)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            plot_path = plots_dir / f"training_curves_{timestamp}.png"
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            logger.info(f"è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ°: {plot_path}")
            plt.close()
            
        except ImportError:
            logger.warning("matplotlibæœªå®‰è£…ï¼Œè·³è¿‡è®­ç»ƒæ›²çº¿ç»˜åˆ¶")
        except Exception as e:
            logger.error(f"ç»˜åˆ¶è®­ç»ƒæ›²çº¿æ—¶å‡ºé”™: {e}")

    def load_checkpoint(self, checkpoint_path=None):
        """åŠ è½½æ£€æŸ¥ç‚¹ï¼Œæ”¯æŒæ¢å¤è®­ç»ƒ"""
        if checkpoint_path is None:
            checkpoint_path = self.best_model_path
        
        if os.path.exists(checkpoint_path):
            checkpoint = torch.load(checkpoint_path, weights_only=False)
            
            # å¤„ç†DataParallelæ¨¡å‹çš„state_dicté”®åä¸åŒ¹é…é—®é¢˜
            model_state_dict = checkpoint['model_state_dict']
            
            is_model_parallel = isinstance(self.model, nn.DataParallel)
            has_module_prefix = any(key.startswith('module.') for key in model_state_dict.keys())
            
            if is_model_parallel and not has_module_prefix:
                model_state_dict = {f'module.{k}': v for k, v in model_state_dict.items()}
            elif not is_model_parallel and has_module_prefix:
                model_state_dict = {k.replace('module.', ''): v for k, v in model_state_dict.items()}
            
            self.model.load_state_dict(model_state_dict)
              
            if 'train_history' in checkpoint:
                self.train_history = checkpoint['train_history']
            
            self.best_val_f1 = checkpoint.get('val_f1', 0.0)
            
            logger.info(f"æˆåŠŸåŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
            logger.info(f"æœ€ä½³éªŒè¯F1: {self.best_val_f1:.4f}")
            return checkpoint.get('epoch', 0)
        else:
            logger.warning(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
            return 0

    def test(self):
        """æµ‹è¯•æ¨¡å‹æ€§èƒ½ - å§‹ç»ˆåŠ è½½æœ€ä½³ä¿å­˜çš„æ¨¡å‹"""
        # åŠ è½½æœ€ä½³æ¨¡å‹
        if os.path.exists(self.best_model_path):
            checkpoint = torch.load(self.best_model_path, map_location=self.device, weights_only=False)
            model_state_dict = checkpoint.get('model_state_dict', checkpoint)
            
            # å¤„ç†DataParallelæ¨¡å‹çš„state_dicté”®åä¸åŒ¹é…é—®é¢˜
            is_model_parallel = isinstance(self.model, nn.DataParallel)
            has_module_prefix = any(key.startswith('module.') for key in model_state_dict.keys())
            
            if is_model_parallel and not has_module_prefix:
                model_state_dict = {f'module.{k}': v for k, v in model_state_dict.items()}
            elif not is_model_parallel and has_module_prefix:
                model_state_dict = {k.replace('module.', ''): v for k, v in model_state_dict.items()}
            
            self.model.load_state_dict(model_state_dict)
            
            best_epoch = checkpoint.get('epoch', 0) + 1
            best_f1 = checkpoint.get('val_f1', 0.0)
            best_loss = checkpoint.get('val_loss', 0.0)
            
            logger.info(f"âœ… åŠ è½½æœ€ä½³æ¨¡å‹ (Epoch {best_epoch})")
            logger.info(f"   éªŒè¯F1: {best_f1:.4f}, éªŒè¯Loss: {best_loss:.4f}")
        else:
            logger.warning(f"æœ€ä½³æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.best_model_path}")
            logger.info("ä½¿ç”¨å½“å‰è®­ç»ƒçŠ¶æ€çš„æ¨¡å‹è¿›è¡Œæµ‹è¯•")
        
        # æµ‹è¯•é›†æµ‹è¯•
        test_results = None
        if self.test_loader:
            logger.info(f"\n--- æœªçŸ¥åè®®æµ‹è¯• - å­—èŠ‚çº§è¾¹ç•Œè¯„ä¼° (å…±è¯†è¯„ä¼°) ---")
            test_results = self.validate(self.test_loader, "Unknown Protocols Test")
            
            logger.info("\nå„åè®®è¯¦ç»†æŒ‡æ ‡:")
            for protocol, metrics in test_results['by_protocol'].items():
                logger.info(f"{protocol:>8} - F1: {metrics['f1']:.4f}, "
                           f"Prec: {metrics['precision']:.4f}, "
                           f"Recall: {metrics['recall']:.4f}")
        
        self.show_protocol_examples()
        
        return test_results

    def show_protocol_examples(self):
        """å±•ç¤ºå„åè®®çš„å…±è¯†é¢„æµ‹æ ¼å¼ - è¯¦ç»†åˆ†æç‰ˆæœ¬"""
        logger.info("\n" + "="*80)
        logger.info("åè®®å…±è¯†æ ¼å¼åˆ†æ")
        logger.info("="*80)

        self.model.eval()
        
        from config import PROTOCOLS
        target_protocols = PROTOCOLS
        
        protocol_consensus = {}
        
        if self.test_loader is None:
            logger.info("æ²¡æœ‰æµ‹è¯•é›†æ•°æ®")
            return

        with torch.no_grad():
            protocol_results = {}
            
            for batch in self.test_loader:
                # ç»Ÿä¸€å¤„ç†è¾“å…¥
                bit_matrices = batch['bit_matrix'].to(self.device)
                labels = batch['labels'].to(self.device)
                protocols = batch['protocols']
                
                attention_mask = (bit_matrices.sum(dim=-1) > 0)
                
                # è·å–ç»“æ„ç‰¹å¾
                structure_features = batch.get('structure_features')
                if structure_features is not None:
                    structure_features = structure_features.to(self.device)
                
                # åªè°ƒç”¨ ColFormerDynamicSGP æ¨¡å‹
                logits = self.model(
                    bit_matrix=bit_matrices,
                    structure_features=structure_features,
                    attention_mask=attention_mask
                )

       

                preds = torch.argmax(logits, dim=2)
                mask = labels != -100

                for i in range(len(protocols)):
                    protocol = protocols[i]
                    sample_mask = mask[i]
                    
                    if sample_mask.any() and protocol in target_protocols:
                        sample_preds = preds[i][sample_mask].cpu().numpy()
                        sample_labels = labels[i][sample_mask].cpu().numpy()
                        sample_bits = bit_matrices[i][sample_mask].cpu().numpy()

                        if protocol not in protocol_results:
                            protocol_results[protocol] = {
                                'preds': [], 'labels': [], 'bits': []
                            }
                        
                        protocol_results[protocol]['preds'].append(sample_preds.tolist())
                        protocol_results[protocol]['labels'].append(sample_labels.tolist())
                        protocol_results[protocol]['bits'].append(sample_bits)

            # è®¡ç®—æ¯ä¸ªåè®®çš„å…±è¯†æ ¼å¼ï¼ˆæŒ‰é•¿åº¦åˆ†ç»„ï¼‰
            for protocol, data in protocol_results.items():
                if len(data['preds']) >= 2:
                    # æŒ‰é•¿åº¦åˆ†ç»„åˆ†æ
                    length_groups = {}
                    for i, pred in enumerate(data['preds']):
                        length = len(pred)
                        if length not in length_groups:
                            length_groups[length] = {'count': 0, 'preds': [], 'labels': [], 'bits': []}
                        length_groups[length]['count'] += 1
                        length_groups[length]['preds'].append(pred)
                        length_groups[length]['labels'].append(data['labels'][i])
                        length_groups[length]['bits'].append(data['bits'][i])
                    
                    protocol_consensus[protocol] = length_groups

        # æ˜¾ç¤ºå„åè®®çš„è¯¦ç»†åˆ†æ
        for protocol in target_protocols:
            if protocol in protocol_consensus:
                logger.info(f"\n--- {protocol.upper()} åè®® ---")
                length_groups = protocol_consensus[protocol]
                
                # æ‰¾åˆ°æ ·æœ¬æ•°æœ€å¤šçš„ç»„
                max_count = 0
                best_group = None
                best_length = None
                
                for length, group in length_groups.items():
                    if group['count'] > max_count and group['count'] >= 2:  # è‡³å°‘2ä¸ªæ ·æœ¬
                        max_count = group['count']
                        best_group = group
                        best_length = length
                
                if best_group:
                    # æ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
                    logger.info(f"æ ·æœ¬æ•°: {max_count}")
                    logger.info(f"æœ‰æ•ˆé•¿åº¦: {best_length}")
                    
                    # è·å–å…±è¯†æ ¼å¼
                    pred_format, true_format, detailed_stats = self._get_detailed_consensus_format(
                        best_group['preds'], best_group['labels'], best_group['bits'][0]
                    )
                    
                    logger.info(f"  é¢„æµ‹æ ¼å¼: {pred_format}")
                    logger.info(f"  çœŸå®æ ¼å¼: {true_format}")
                    
                    # æ˜¾ç¤ºè¯¦ç»†çš„æŠ•ç¥¨ç»Ÿè®¡
                    self._show_voting_statistics(detailed_stats, max_count)
                    
                    # æ˜¾ç¤ºè¾¹ç•Œä½ç½®æ‘˜è¦
                    self._show_boundary_summary(detailed_stats)
                    
                else:
                    logger.info("æ²¡æœ‰è¶³å¤Ÿæ ·æœ¬çš„é•¿åº¦ç»„è¿›è¡Œåˆ†æ")
            
  

    def _get_detailed_consensus_format(self, predictions, ground_truths, sample_bits):
        """ä¸ºç‰¹å®šé•¿åº¦ç»„è®¡ç®—è¯¦ç»†çš„å…±è¯†æ ¼å¼å’Œç»Ÿè®¡ä¿¡æ¯"""
        if len(predictions) < 2:
            return "æ ·æœ¬ä¸è¶³", "æ ·æœ¬ä¸è¶³", None
        
        length = len(predictions[0])
        num_predictions = len(predictions)
        
        # æ”¶é›†æ¯ä¸ªä½ç½®çš„æŠ•ç¥¨ç»Ÿè®¡
        position_stats = []
        consensus_pred_seq = []
        consensus_gt_seq = []
        
        for pos in range(length):
            pred_votes = sum(seq[pos] for seq in predictions)
            gt_votes = sum(seq[pos] for seq in ground_truths)
            
            pred_boundary = pred_votes >= (num_predictions * 0.5)
            gt_boundary = gt_votes >= (num_predictions * 0.5)
            
            consensus_pred_seq.append(1 if pred_boundary else 0)
            consensus_gt_seq.append(1 if gt_boundary else 0)
            
            position_stats.append({
                'position': pos,
                'pred_votes': pred_votes,
                'gt_votes': gt_votes,
                'pred_boundary': pred_boundary,
                'gt_boundary': gt_boundary
            })
        
        # é‡æ„åå…­è¿›åˆ¶æ•°æ®
        hex_data = ""
        for i in range(length):
            byte_bits = sample_bits[i]
            byte_value = 0
            for j in range(8):
                if byte_bits[j] > 0.5:
                    byte_value |= (1 << (7-j))
            hex_data += f"{byte_value:02x}"
        
        pred_format = self._format_fields(hex_data, consensus_pred_seq)
        true_format = self._format_fields(hex_data, consensus_gt_seq)
        
        detailed_stats = {
            'position_stats': position_stats,
            'consensus_pred_seq': consensus_pred_seq,
            'consensus_gt_seq': consensus_gt_seq,
            'total_samples': num_predictions,
            'length': length
        }
        
        return pred_format, true_format, detailed_stats

    def _show_voting_statistics(self, detailed_stats, total_samples):
        """æ˜¾ç¤ºè¯¦ç»†çš„æŠ•ç¥¨ç»Ÿè®¡è¡¨æ ¼"""
        logger.info(f"\nä½ç½®æŠ•ç¥¨ç»Ÿè®¡ (æ€»æ ·æœ¬æ•°: {total_samples}):")
        logger.info("ä½ç½®  é¢„æµ‹æŠ•ç¥¨  çœŸå®æŠ•ç¥¨  é¢„æµ‹è¾¹ç•Œ  çœŸå®è¾¹ç•Œ")
        logger.info("-" * 50)
        
        position_stats = detailed_stats['position_stats']
        length = detailed_stats['length']
        
        # æ˜¾ç¤ºå‰20ä¸ªä½ç½®ï¼Œå¦‚æœè¶…è¿‡åˆ™æ˜¾ç¤ºçœç•¥
        display_limit = 20
        for i, stats in enumerate(position_stats[:display_limit]):
            pred_votes = stats['pred_votes']
            gt_votes = stats['gt_votes']
            pred_boundary = "æ˜¯" if stats['pred_boundary'] else "å¦"
            gt_boundary = "æ˜¯" if stats['gt_boundary'] else "å¦"
            
            logger.info(f"{stats['position']:>4}   {pred_votes:>6.1f}     {gt_votes:>6.1f}      {pred_boundary:>2}       {gt_boundary:>2}")
        
        if length > display_limit:
            logger.info(f"... (è¿˜æœ‰ {length - display_limit} ä¸ªä½ç½®)")

    def _show_boundary_summary(self, detailed_stats):
        """æ˜¾ç¤ºè¾¹ç•Œä½ç½®æ‘˜è¦"""
        position_stats = detailed_stats['position_stats']
        
        # æå–è¾¹ç•Œä½ç½®
        pred_boundaries = [stats['position'] for stats in position_stats if stats['pred_boundary']]
        gt_boundaries = [stats['position'] for stats in position_stats if stats['gt_boundary']]
        
        # è®¡ç®—æŠ•ç¥¨å¼ºåº¦ç»Ÿè®¡
        pred_votes = [stats['pred_votes'] for stats in position_stats]
        gt_votes = [stats['gt_votes'] for stats in position_stats]
        
        logger.info("\nè¾¹ç•Œä½ç½®æ‘˜è¦:")
        logger.info(f"  é¢„æµ‹è¾¹ç•Œä½ç½®: {pred_boundaries}")
        logger.info(f"  çœŸå®è¾¹ç•Œä½ç½®: {gt_boundaries}")
        
        if pred_votes:
            logger.info(f"  é¢„æµ‹æŠ•ç¥¨å¼ºåº¦: å¹³å‡={np.mean(pred_votes):.1f}, "
                       f"æœ€å¤§={np.max(pred_votes):.1f}, æœ€å°={np.min(pred_votes):.1f}")
        
        if gt_votes:
            logger.info(f"  çœŸå®æŠ•ç¥¨å¼ºåº¦: å¹³å‡={np.mean(gt_votes):.1f}, "
                       f"æœ€å¤§={np.max(gt_votes):.1f}, æœ€å°={np.min(gt_votes):.1f}")
        
        # è®¡ç®—åŒ¹é…åº¦
        pred_set = set(pred_boundaries)
        gt_set = set(gt_boundaries)
        
        correct_boundaries = len(pred_set & gt_set)
        missed_boundaries = len(gt_set - pred_set)
        false_boundaries = len(pred_set - gt_set)
        
        logger.info(f"  è¾¹ç•ŒåŒ¹é…æƒ…å†µ: æ­£ç¡®={correct_boundaries}, "
                   f"é—æ¼={missed_boundaries}, è¯¯æŠ¥={false_boundaries}")
        
        # è®¡ç®—è¾¹ç•Œçº§åˆ«çš„ç²¾ç¡®ç‡å’Œå¬å›ç‡
        if len(pred_set) > 0:
            boundary_precision = correct_boundaries / len(pred_set)
            logger.info(f"  è¾¹ç•Œç²¾ç¡®ç‡: {boundary_precision:.3f}")
        
        if len(gt_set) > 0:
            boundary_recall = correct_boundaries / len(gt_set)
            logger.info(f"  è¾¹ç•Œå¬å›ç‡: {boundary_recall:.3f}")

    

    def _get_consensus_format_for_group(self, predictions, ground_truths, sample_bits):
        """ä¸ºç‰¹å®šé•¿åº¦ç»„è®¡ç®—å…±è¯†æ ¼å¼"""
        if len(predictions) < 2:
            return "æ ·æœ¬ä¸è¶³", "æ ·æœ¬ä¸è¶³"
        
        length = len(predictions[0])
        num_predictions = len(predictions)
        consensus_pred_seq = []
        consensus_gt_seq = []
        
        for pos in range(length):
            pred_votes = sum(seq[pos] for seq in predictions)
            consensus_pred_seq.append(1 if pred_votes >= (num_predictions * 0.5) else 0)
            
            gt_votes = sum(seq[pos] for seq in ground_truths)
            consensus_gt_seq.append(1 if gt_votes >= (num_predictions * 0.5) else 0)
        
        # é‡æ„åå…­è¿›åˆ¶æ•°æ®
        hex_data = ""
        for i in range(length):
            byte_bits = sample_bits[i]
            byte_value = 0
            for j in range(8):
                if byte_bits[j] > 0.5:
                    byte_value |= (1 << (7-j))
            hex_data += f"{byte_value:02x}"
        
        pred_format = self._format_fields(hex_data, consensus_pred_seq)
        true_format = self._format_fields(hex_data, consensus_gt_seq)
        
        return pred_format, true_format

    def _format_fields(self, hex_data, boundary_seq):
        """æ ¹æ®è¾¹ç•Œåºåˆ—æ ¼å¼åŒ–å­—æ®µ"""
        boundary_positions = [i for i, label in enumerate(boundary_seq) if label == 1]
        
        if not boundary_positions:
            return hex_data  # æ²¡æœ‰è¾¹ç•Œï¼Œæ•´ä¸ªæ•°æ®ä½œä¸ºä¸€ä¸ªå­—æ®µ
        
        fields = []
        start_pos = 0
        
        for boundary_pos in boundary_positions:
            end_pos = boundary_pos + 1
            field_hex = hex_data[start_pos*2:end_pos*2]
            if field_hex:
                fields.append(field_hex)
            start_pos = end_pos
        
        # å¤„ç†æœ€åä¸€ä¸ªå­—æ®µ
        if start_pos < len(boundary_seq):
            remaining_hex = hex_data[start_pos*2:]
            if remaining_hex:
                fields.append(remaining_hex)
        
        return " | ".join(fields) if fields else hex_data

def run_cross_validation():
    """è¿è¡Œç•™ä¸€æ³•äº¤å‰éªŒè¯å®éªŒ"""
    from config import EXPERIMENT_CONFIG
    
    target_protocols = EXPERIMENT_CONFIG['target_protocols']
    all_results = {}
    
    logger.info("="*80)
    logger.info("å¼€å§‹ç•™ä¸€æ³•äº¤å‰éªŒè¯å®éªŒ")
    logger.info(f"ç›®æ ‡åè®®: {target_protocols}")
    logger.info("="*80)
    logger.info("ä½¿ç”¨å…±è¯†è¯„ä¼°æ¨¡å¼")
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦é¢„å¤„ç†æ•°æ®
    preprocessor = NetworkPacketPreprocessor()
    dataset_cache_dir = Path("data/dataset")
    
    if not dataset_cache_dir.exists() or not any(dataset_cache_dir.iterdir()):
        logger.info("æœªæ‰¾åˆ°é¢„å¤„ç†çš„æ•°æ®é›†ï¼Œå¼€å§‹é¢„å¤„ç†...")
        preprocessor.save_cross_validation_datasets()
    else:
        logger.info("å‘ç°å·²é¢„å¤„ç†çš„æ•°æ®é›†ï¼Œç›´æ¥ä½¿ç”¨")
    
    model_name = get_model_name()
    logger.info(f"ä½¿ç”¨æ¨¡å‹: {model_name}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å·²è®­ç»ƒçš„æ¨¡å‹
    checkpoint_dir = Path(PATH_CONFIG['checkpoint_dir']) / model_name
    existing_models = []
    if checkpoint_dir.exists():
        existing_models = [f for f in checkpoint_dir.glob("*_best.pth") if f.is_file()]
    
    retrain_all = False
    if existing_models:
        logger.info(f"\nå‘ç° {len(existing_models)} ä¸ªå·²è®­ç»ƒçš„æ¨¡å‹:")
        for model_file in existing_models:
            protocol = model_file.stem.replace('_best', '')
            logger.info(f"  - {protocol}")
    
        retrain_all = True  # é»˜è®¤é‡æ–°è®­ç»ƒæ‰€æœ‰æ¨¡å‹ï¼Œä¾¿äºæµ‹è¯•
    
    for i, test_protocol in enumerate(target_protocols):
        logger.info(f"\n{'='*60}")
        logger.info(f"å®éªŒ {i+1}/{len(target_protocols)}: æµ‹è¯•åè®® {test_protocol}")
        logger.info(f"{'='*60}")
        
        # åŠ è½½é¢„å¤„ç†çš„æ•°æ®é›†
        train_dataset, val_dataset, test_dataset = preprocessor.load_cross_validation_dataset(
            test_protocol
        )

        if train_dataset is None or val_dataset is None or test_dataset is None:
            logger.error(f"åŠ è½½åè®® {test_protocol} æ•°æ®é›†å¤±è´¥ï¼Œè·³è¿‡")
            continue

        # ç»Ÿä¸€ä½¿ç”¨åè®®åˆ†ç»„æ•°æ®åŠ è½½å™¨ - é€‚é…åŒåè®®æµé‡åœºæ™¯
        logger.info("ä½¿ç”¨åè®®åˆ†ç»„æ•°æ®åŠ è½½å™¨ (é€‚é…åŒåè®®æµé‡åœºæ™¯)")
        train_loader, val_loader, test_loader = create_data_loaders(
            train_dataset, val_dataset, test_dataset, 
            batch_size=TRAINING_CONFIG['base_batch_size']
        )
        
        # æ›´æ–°æ¨¡å‹ä¿å­˜è·¯å¾„ - æŒ‰æ¨¡å‹ç±»å‹åˆ†ç±»
        checkpoint_dir = Path(PATH_CONFIG['checkpoint_dir']) / model_name
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
        model_path = checkpoint_dir / f"{test_protocol}_best.pth"
        PATH_CONFIG['best_model_path'] = str(model_path)
        
        logger.info(f"æ¨¡å‹ä¿å­˜è·¯å¾„: {model_path}")
        
        # è®¾ç½®è®¾å¤‡
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åˆ›å»ºæ¨¡å‹
        model = create_model().to(device)
        if torch.cuda.device_count() > 1:
            model = nn.DataParallel(model)
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = ProtocolTrainer(model, train_loader, val_loader, test_loader, device)
        
        # æ ¹æ®ç”¨æˆ·é€‰æ‹©å†³å®šæ˜¯å¦è®­ç»ƒ
        if os.path.exists(model_path) and not retrain_all:
            logger.info(f"ä½¿ç”¨å·²è®­ç»ƒæ¨¡å‹: {model_path}")
            test_results = trainer.test()
        else:
            if os.path.exists(model_path):
                logger.info(f"é‡æ–°è®­ç»ƒæ¨¡å‹ (æœªçŸ¥åè®®: {test_protocol})")
            else:
                logger.info(f"å¼€å§‹è®­ç»ƒ (æœªçŸ¥åè®®: {test_protocol})")
            trainer.train(num_epochs=TRAINING_CONFIG['num_epochs'])
            test_results = trainer.test()
        
        if test_results:
            all_results[test_protocol] = test_results['overall']
        
    
    # è¾“å‡ºæœ€ç»ˆç»“æœæ±‡æ€»
    if all_results:
        logger.info("\n" + "="*80)
        logger.info(f"ç•™ä¸€æ³•äº¤å‰éªŒè¯æœ€ç»ˆç»“æœæ±‡æ€» - {model_name}")
        logger.info("="*80)
        
        f1_scores = []
        precision_scores = []
        recall_scores = []
        
        for protocol, metrics in all_results.items():
            f1_scores.append(metrics['f1'])
            precision_scores.append(metrics['precision'])
            recall_scores.append(metrics['recall'])
            logger.info(f"{protocol:>8} - F1: {metrics['f1']:.4f}, "
                       f"Precision: {metrics['precision']:.4f}, "
                       f"Recall: {metrics['recall']:.4f}")
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_f1 = np.mean(f1_scores)
        avg_precision = np.mean(precision_scores)
        avg_recall = np.mean(recall_scores)
        std_f1 = np.std(f1_scores)
        
        logger.info("-" * 80)
        logger.info(f"{'å¹³å‡':>8} - F1: {avg_f1:.4f}Â±{std_f1:.4f}, "
                   f"Precision: {avg_precision:.4f}, "
                   f"Recall: {avg_recall:.4f}")
        logger.info("="*80)



def main():
    # é¦–å…ˆè®¾ç½®éšæœºç§å­
    set_seed(DATA_CONFIG['random_seed'])
    
    logger.info("="*80)
    logger.info("åè®®è¾¹ç•Œæ£€æµ‹è®­ç»ƒå¼€å§‹")
    logger.info(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"éšæœºç§å­: {DATA_CONFIG['random_seed']}")
    logger.info("="*80)
    

    
    selected_gpus = TRAINING_CONFIG['gpu_ids'] 
    os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(map(str, selected_gpus))
    logger.info(f"è®¾ç½®CUDA_VISIBLE_DEVICES: {os.environ['CUDA_VISIBLE_DEVICES']}")
    logger.info(f"ä½¿ç”¨GPU: {selected_gpus}")
    logger.info(f"ä½¿ç”¨æ¨¡å‹: ColFormerDynamicSGP")
    
    # åªæ˜¾ç¤º ColFormerDynamicSGP çš„é…ç½®
    logger.info(f"æ¨¡å‹é…ç½®å‚æ•°: {COLFORMER_DYNAMIC_SGP_CONFIG}")
    
    # é‡æ–°åˆå§‹åŒ–CUDAä¸Šä¸‹æ–‡
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    # è®¾ç½®è®¾å¤‡å’Œå¤šGPU
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    gpu_count = torch.cuda.device_count()
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    logger.info(f"PyTorchå¯è§GPUæ•°é‡: {gpu_count}")
    
    # ç›´æ¥è¿è¡Œç•™ä¸€æ³•äº¤å‰éªŒè¯
    logger.info("å¼€å§‹ç•™ä¸€æ³•äº¤å‰éªŒè¯æ¨¡å¼...")
    run_cross_validation()

if __name__ == "__main__":
    main()
