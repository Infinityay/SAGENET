import json
import re
import subprocess
import os
from pathlib import Path
from typing import Dict, List, Optional, Generator, Any, Union

class ProtocolJsonHandler:
    """
    å¤„ç†å·²é¢„å¤„ç†çš„åè®® JSON æ•°æ®
    é€‚ç”¨äºå·²æœ‰ json/ å’Œ jsonraw/ æ–‡ä»¶å¤¹çš„æƒ…å†µ
    """
    
    def __init__(self, json_folder: Path, jsonraw_folder: Path, include_protocols: List[str] = None):
        """
        åˆå§‹åŒ–å¤„ç†å™¨
        
        Args:
            json_folder: åŒ…å«åè®® JSON æ–‡ä»¶çš„æ–‡ä»¶å¤¹è·¯å¾„
            jsonraw_folder: åŒ…å«åè®®åŸå§‹ JSON æ–‡ä»¶çš„æ–‡ä»¶å¤¹è·¯å¾„  
            include_protocols: è¦åŒ…å«çš„åè®®åˆ—è¡¨ï¼ŒNone è¡¨ç¤ºåŒ…å«æ‰€æœ‰åè®®
        """
        self.json_folder = Path(json_folder)
        self.jsonraw_folder = Path(jsonraw_folder)
        self.include_protocols = [p.lower() for p in include_protocols] if include_protocols else None
        
        # è·å–å¯ç”¨çš„åè®®æ–‡ä»¶
        self.available_protocols = self._get_available_protocols()
        
        # ç”¨äºè·Ÿè¸ªæ¯ä¸ªåè®®å®é™…ä½¿ç”¨çš„åŒ…ID
        self.used_packet_ids = {}
    
    def _get_available_protocols(self) -> List[str]:
        """è·å–å¯ç”¨çš„åè®®åˆ—è¡¨"""
        protocols = []
        for json_file in self.json_folder.glob("*.json"):
            protocol_name = json_file.stem
            # æ£€æŸ¥å¯¹åº”çš„ raw æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            raw_file = self.jsonraw_folder / f"{protocol_name}_raw.json"
            if raw_file.exists():
                protocols.append(protocol_name)
                
                # å¦‚æœæ˜¯modbusåè®®ï¼ŒåŒæ—¶æ·»åŠ mbtcpä½œä¸ºå¯ç”¨åè®®
                if protocol_name.lower() == 'modbus':
                    protocols.append('mbtcp')
        
        return protocols
    
    def _validate_split_positions(self, split_pos: Dict, data_len: int) -> Dict:
        """
        éªŒè¯åˆ†å‰²ä½ç½®çš„æœ‰æ•ˆæ€§
        
        Args:
            split_pos: åŒ…å« b_pos å’Œ e_pos çš„å­—å…¸
            data_len: æ•°æ®é•¿åº¦ï¼ˆå­—èŠ‚ï¼‰
        
        Returns:
            éªŒè¯ç»“æœå­—å…¸
        """
        b_pos = split_pos['b_pos']
        e_pos = split_pos['e_pos']
        max_bits = data_len * 8
        
        validation = {
            'is_valid': True,
            'warnings': [],
            'errors': [],
            'stats': {}
        }
        
        # 1. åŸºæœ¬æ•°é‡æ£€æŸ¥
        validation['stats']['b_pos_count'] = len(b_pos)
        validation['stats']['e_pos_count'] = len(e_pos)
        
        if len(b_pos) != len(e_pos):
            validation['errors'].append(f"Begin positions ({len(b_pos)}) != End positions ({len(e_pos)})")
            validation['is_valid'] = False
        
        # 2. ä½ç½®æœ‰æ•ˆæ€§æ£€æŸ¥
        for i, pos in enumerate(b_pos):
            if pos < 0:
                validation['errors'].append(f"Begin position {i} is negative: {pos}")
                validation['is_valid'] = False
            elif pos >= max_bits:
                validation['errors'].append(f"Begin position {i} exceeds data length: {pos} >= {max_bits}")
                validation['is_valid'] = False
        
        for i, pos in enumerate(e_pos):
            if pos < 0:
                validation['errors'].append(f"End position {i} is negative: {pos}")
                validation['is_valid'] = False
            elif pos > max_bits:
                validation['errors'].append(f"End position {i} exceeds data length: {pos} > {max_bits}")
                validation['is_valid'] = False
        
        # 3. å¦‚æœåŸºæœ¬æ£€æŸ¥é€šè¿‡ï¼Œè¿›è¡Œé€»è¾‘æ£€æŸ¥
        if validation['is_valid'] and len(b_pos) == len(e_pos):
            # æ£€æŸ¥æ¯ä¸ªå­—æ®µçš„å¼€å§‹-ç»“æŸä½ç½®å¯¹åº”å…³ç³»
            fields = []
            for i in range(len(b_pos)):
                start = b_pos[i]
                # æ‰¾åˆ°å¯¹åº”çš„ç»“æŸä½ç½®ï¼ˆæœ€å°çš„å¤§äºstartçš„endä½ç½®ï¼‰
                end = None
                for e in e_pos:
                    if e > start:
                        end = e
                        break
                
                if end is None:
                    validation['warnings'].append(f"Begin position {start} has no matching end position")
                else:
                    fields.append((start, end))
            
            # æŒ‰å¼€å§‹ä½ç½®æ’åº
            fields.sort()
            validation['stats']['valid_fields'] = len(fields)
            
            # 4. æ£€æŸ¥å­—æ®µé‡å å’Œé—´éš™
            for i in range(len(fields) - 1):
                current_end = fields[i][1]
                next_start = fields[i + 1][0]
                
                if current_end > next_start:
                    validation['warnings'].append(
                        f"Field overlap: field {i} ends at {current_end}, field {i+1} starts at {next_start}"
                    )
                elif current_end < next_start:
                    gap_size = next_start - current_end
                    if gap_size > 8:  # è¶…è¿‡1å­—èŠ‚çš„é—´éš™å¯èƒ½æœ‰é—®é¢˜
                        validation['warnings'].append(
                            f"Large gap ({gap_size} bits) between field {i} and {i+1}"
                        )
            
            # 5. æ£€æŸ¥ç©ºå­—æ®µ
            empty_fields = [(start, end) for start, end in fields if start == end]
            if empty_fields:
                validation['warnings'].append(f"Found {len(empty_fields)} empty fields: {empty_fields}")
            
            # 6. è¦†ç›–ç‡ç»Ÿè®¡
            if fields:
                total_covered_bits = sum(end - start for start, end in fields if end > start)
                coverage_percentage = (total_covered_bits / max_bits) * 100
                validation['stats']['coverage_percentage'] = round(coverage_percentage, 2)
                validation['stats']['total_covered_bits'] = total_covered_bits
                validation['stats']['max_bits'] = max_bits
                
                if coverage_percentage < 50:
                    validation['warnings'].append(f"Low field coverage: {coverage_percentage:.1f}%")
                elif coverage_percentage > 100:
                    validation['errors'].append(f"Field coverage exceeds 100%: {coverage_percentage:.1f}%")
                    validation['is_valid'] = False
        
        # 7. å­—èŠ‚å¯¹é½æ£€æŸ¥
        unaligned_positions = [pos for pos in b_pos + e_pos if pos % 8 != 0]
        if unaligned_positions:
            validation['stats']['unaligned_positions'] = len(unaligned_positions)
            # è¿™é€šå¸¸æ˜¯æ­£å¸¸çš„ï¼ˆä½å­—æ®µï¼‰ï¼Œæ‰€ä»¥åªæ˜¯ç»Ÿè®¡è€Œä¸æ˜¯è­¦å‘Š
        
        return validation
    
    @staticmethod
    def _select_first(*items, fun=lambda x: x is not None):
        """ä»å¤šä¸ªé€‰é¡¹ä¸­é€‰æ‹©ç¬¬ä¸€ä¸ªæ»¡è¶³æ¡ä»¶çš„"""
        for item in items:
            if fun(item):
                return item
        return None
    
    @staticmethod
    def _get_value(obj: Dict, keys: List[str], default=None):
        """å®‰å…¨åœ°ä»åµŒå¥—å­—å…¸ä¸­è·å–å€¼"""
        try:
            result = obj
            for key in keys:
                result = result[key]
            return result
        except (KeyError, TypeError):
            return default
    
    @staticmethod
    def _parse_bitmask(field_info: List) -> tuple:
        """è§£æä½æ©ç ä¿¡æ¯"""
        if len(field_info) < 4:
            return -1, -1
        
        mask_bin = f'{field_info[3]:0>{field_info[2] * 8}b}'
        l_pos = mask_bin.find('1')
        r_pos = mask_bin.rfind('1')
        
        if l_pos >= 0 and r_pos >= 0:
            return l_pos, r_pos + 1
        else:
            return -1, -1
    
    def _extract_basic_info(self, packet: Dict) -> Dict:
        """ä»æ•°æ®åŒ…ä¸­æå–åŸºæœ¬ä¿¡æ¯"""
        time_epoch = self._get_value(packet, ['_source', 'layers', 'frame', 'frame.time_epoch'])
        
        # æå–æºåœ°å€
        src_addr = self._select_first(
            self._get_value(packet, ['_source', 'layers', 'ip', 'ip.src']),
            self._get_value(packet, ['_source', 'layers', 'eth', 'eth.src']),
            self._get_value(packet, ['_source', 'layers', 'zbee_nwk', 'zbee_nwk.src']),
            self._get_value(packet, ['_source', 'layers', 'wpan', 'wpan.src16'])
        )
        
        # æå–ç›®æ ‡åœ°å€
        dst_addr = self._select_first(
            self._get_value(packet, ['_source', 'layers', 'ip', 'ip.dst']),
            self._get_value(packet, ['_source', 'layers', 'eth', 'eth.dst']),
            self._get_value(packet, ['_source', 'layers', 'zbee_nwk', 'zbee_nwk.dst']),
            self._get_value(packet, ['_source', 'layers', 'wpan', 'wpan.dst16'])
        )
        
        # æå–æºç«¯å£
        src_port = self._select_first(
            self._get_value(packet, ['_source', 'layers', 'tcp', 'tcp.srcport']),
            self._get_value(packet, ['_source', 'layers', 'udp', 'udp.srcport'])
        )
        
        # æå–ç›®æ ‡ç«¯å£
        dst_port = self._select_first(
            self._get_value(packet, ['_source', 'layers', 'tcp', 'tcp.dstport']),
            self._get_value(packet, ['_source', 'layers', 'udp', 'udp.dstport'])
        )
        
        # æå–æµID
        stream_id = self._select_first(
            self._get_value(packet, ['_source', 'layers', 'tcp', 'tcp.stream']),
            self._get_value(packet, ['_source', 'layers', 'udp', 'udp.stream'])
        )
        
        return {
            'time': time_epoch,
            'src': (src_addr, src_port),
            'dst': (dst_addr, dst_port),
            'stream': stream_id
        }
    
    def _extract_split_position(self, start_bias: int, data_len: int, raw_layer: Dict) -> Dict:
        """æå–å­—æ®µåˆ†å‰²ä½ç½®ä¿¡æ¯ï¼Œä¿ç•™æœ€ç»†ç²’åº¦çš„å­—æ®µåˆ’åˆ†"""
        def traverse_field_info(json_node: Union[Dict, List], field_info: Dict, prefix: str = ''):
            """é€’å½’éå†å­—æ®µä¿¡æ¯ (ä¿®æ­£ä»¥å¤„ç†åˆ—è¡¨)"""
            if isinstance(json_node, dict):
                for field, field_data in json_node.items():
                    # å…¼å®¹ "Queries" ä¸‹çš„åŠ¨æ€key
                    current_prefix = f"{prefix}.{field}" if prefix and not field.endswith(":") else field
                    traverse_field_info(field_data, field_info, prefix=current_prefix)
            elif isinstance(json_node, list):
                # è¿™æ˜¯ä¸€ä¸ªå¶å­èŠ‚ç‚¹
                field_info[prefix] = json_node

        # æ”¶é›†æ‰€æœ‰å­—æ®µä¿¡æ¯
        field_list = {}
        traverse_field_info(raw_layer, field_list)

        # æå–æ‰€æœ‰å­—æ®µçš„ä½ç½®ä¿¡æ¯ï¼ŒåŒ…æ‹¬å±‚çº§å…³ç³»
        all_positions = []
        max_bits = data_len * 8

        for field_name, field_info in field_list.items():
            # è·³è¿‡ç©ºå­—æ®µæˆ–æ ¼å¼ä¸æ­£ç¡®çš„å­—æ®µ
            if len(field_info) < 3 or not isinstance(field_info[1], int) or not isinstance(field_info[2], int):
                continue
            
            # è·³è¿‡é•¿åº¦ä¸º0çš„å­—æ®µ
            if field_info[2] == 0:
                continue

            # è¿‡æ»¤ä¸åœ¨å½“å‰åè®®æ•°æ®èŒƒå›´å†…çš„å­—æ®µ
            if field_info[1] < start_bias:
                continue
            
            # è®¡ç®—å­—èŠ‚çº§åˆ«çš„ç›¸å¯¹ä½ç½®
            byte_start = field_info[1] - start_bias
            byte_end = byte_start + field_info[2]
            
            # ç¡®ä¿ä¸è¶…å‡ºæ•°æ®èŒƒå›´
            if byte_start >= data_len:
                continue
            
            # å¦‚æœç»“æŸä½ç½®è¶…å‡ºèŒƒå›´ï¼Œåˆ™æˆªæ–­
            if byte_end > data_len:
                byte_end = data_len

            # è§£æä½æ©ç 
            bit_start, bit_end = self._parse_bitmask(field_info)

            if bit_start >= 0 and bit_end >= 0:
                # ä½å­—æ®µ
                start_pos = byte_start * 8 + bit_start
                end_pos = byte_start * 8 + bit_end
                
                # ç¡®ä¿ä½æ©ç ä¸è¶…å‡ºå­—æ®µèŒƒå›´
                if end_pos > byte_end * 8:
                    end_pos = byte_end * 8
            else:
                # å­—èŠ‚å­—æ®µ
                start_pos = byte_start * 8
                end_pos = byte_end * 8
            
            # æœ€ç»ˆè¾¹ç•Œæ£€æŸ¥
            if start_pos >= max_bits or end_pos > max_bits or start_pos >= end_pos:
                continue
            
            # å­˜å‚¨å­—æ®µä¿¡æ¯ï¼ŒåŒ…æ‹¬åç§°å’Œå±‚çº§æ·±åº¦ï¼ˆé€šè¿‡ç‚¹çš„æ•°é‡åˆ¤æ–­ï¼‰
            depth = field_name.count('.')
            all_positions.append({
                'start': start_pos,
                'end': end_pos,
                'name': field_name,
                'depth': depth
            })
        
        # æŒ‰å¼€å§‹ä½ç½®å’Œç»“æŸä½ç½®æ’åº
        all_positions.sort(key=lambda p: (p['start'], p['end']))
        
        # æ‰¾å‡ºæœ€ç»†ç²’åº¦çš„å­—æ®µåˆ’åˆ†
        # å°†æ•°æ®èŒƒå›´åˆ’åˆ†ä¸ºä¸é‡å çš„åŒºé—´
        intervals = []
        
        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆå­—æ®µï¼Œè¿”å›ç©ºç»“æœ
        if not all_positions:
            result = {
                'b_pos': [],
                'e_pos': [],
                'detailed_intervals': []
            }
            validation_result = self._validate_split_positions(result, data_len)
            result['validation'] = validation_result
            return result
        
        current_positions = sorted([(p['start'], 'start', i) for i, p in enumerate(all_positions)] + 
                                [(p['end'], 'end', i) for i, p in enumerate(all_positions)])
        
        # ä½¿ç”¨æ‰«æçº¿ç®—æ³•æ‰¾å‡ºæ‰€æœ‰ä¸é‡å çš„åŒºé—´
        open_intervals = set()
        last_pos = None
        
        for pos, event_type, idx in current_positions:
            # ç¡®ä¿ä½ç½®åœ¨æœ‰æ•ˆèŒƒå›´å†…
            if pos > max_bits:
                continue
                
            # å¦‚æœä½ç½®å˜åŒ–ä¸”æœ‰å¼€æ”¾çš„åŒºé—´ï¼Œåˆ™æ·»åŠ ä¸€ä¸ªåŒºé—´
            if last_pos is not None and pos > last_pos and open_intervals:
                # åœ¨å¼€æ”¾åŒºé—´ä¸­æ‰¾å‡ºæ·±åº¦æœ€å¤§çš„ï¼ˆæœ€ç»†ç²’åº¦çš„ï¼‰
                deepest_idx = max(open_intervals, key=lambda i: all_positions[i]['depth'])
                intervals.append((last_pos, pos, deepest_idx))
            
            # æ›´æ–°å¼€æ”¾åŒºé—´é›†åˆ
            if event_type == 'start':
                open_intervals.add(idx)
            else:
                if idx in open_intervals:
                    open_intervals.remove(idx)
            
            last_pos = pos
        
        # ä»åŒºé—´ç”Ÿæˆä¸é‡å çš„ b_pos å’Œ e_pos
        b_pos = []
        e_pos = []
        
        for start, end, _ in intervals:
            if start not in b_pos:
                b_pos.append(start)
            if end not in e_pos:
                e_pos.append(end)
        
        # ç¡®ä¿ b_pos å’Œ e_pos æ˜¯æœ‰åºçš„
        b_pos.sort()
        e_pos.sort()
        
        result = {
            'b_pos': b_pos,
            'e_pos': e_pos,
            'detailed_intervals': [(start, end, all_positions[idx]['name']) for start, end, idx in intervals]
        }

        # æ·»åŠ ä½ç½®éªŒè¯
        validation_result = self._validate_split_positions(result, data_len)
        result['validation'] = validation_result

        return result
    
    def load_protocol_data(self, protocol: str) -> tuple:
        """åŠ è½½æŒ‡å®šåè®®çš„ JSON å’ŒåŸå§‹æ•°æ®"""
        # å¯¹äºmbtcpåè®®ï¼Œå®é™…åŠ è½½modbusçš„æ•°æ®æ–‡ä»¶
        actual_protocol = 'modbus' if protocol.lower() == 'mbtcp' else protocol
        
        json_file = self.json_folder / f"{actual_protocol}.json"
        raw_file = self.jsonraw_folder / f"{actual_protocol}_raw.json"
        
        if not json_file.exists():
            raise FileNotFoundError(f"JSON file not found: {json_file}")
        if not raw_file.exists():
            raise FileNotFoundError(f"Raw JSON file not found: {raw_file}")
        
        print(f"Loading {protocol} data from {actual_protocol} files...")
        
        try:
            with json_file.open('r', encoding='utf-8', errors='ignore') as f:
                json_data = json.load(f)
        except json.JSONDecodeError:
            # Fallback to latin-1 if UTF-8 fails
            with json_file.open('r', encoding='latin-1') as f:
                json_data = json.load(f)
        
        try:
            with raw_file.open('r', encoding='utf-8', errors='ignore') as f:
                raw_data = json.load(f)
        except json.JSONDecodeError:
            # Fallback to latin-1 if UTF-8 fails
            with raw_file.open('r', encoding='latin-1') as f:
                raw_data = json.load(f)
        
        print(f"Loaded {len(json_data)} packets for {protocol}")
        return json_data, raw_data

    
    def extract_protocol_splits(self, protocol: str) -> Generator[Dict, None, None]:
        """æå–æŒ‡å®šåè®®çš„æ•°æ®åˆ†å‰²ä¿¡æ¯"""
        if self.include_protocols and protocol.lower() not in self.include_protocols:
            print(f"Skipping protocol {protocol} (not in include list)")
            return
        
        try:
            json_data, raw_data = self.load_protocol_data(protocol)
        except FileNotFoundError as e:
            print(f"Error loading {protocol}: {e}")
            return
        
        # å¯¹MODBUSåè®®ä½¿ç”¨ç‰¹æ®Šçš„ä¼˜å…ˆçº§å¤„ç†
        if protocol.lower() == 'modbus':
            yield from self._extract_modbus_with_priority(json_data, raw_data, protocol)
            return
        
        # å¯¹MBTCPåè®®çš„ç‰¹æ®Šå¤„ç† - ä»modbusæ•°æ®ä¸­æå–mbtcpå±‚ä¿¡æ¯
        if protocol.lower() == 'mbtcp':
            yield from self._extract_mbtcp_from_modbus(json_data, raw_data)
            return
        
        # å…¶ä»–åè®®ä½¿ç”¨ç»Ÿä¸€çš„å¤„ç†é€»è¾‘
        for pkt_id, (pkt, pkt_raw) in enumerate(zip(json_data, raw_data)):
            info = self._extract_basic_info(pkt)
            info['pkt_id'] = pkt_id
            info['protocol'] = protocol
            
            layers = pkt_raw.get('_source', {}).get('layers', {})
            
            if protocol in layers and f"{protocol}_raw" in layers:
                layer_raw_data = layers[f"{protocol}_raw"]
                
                if isinstance(layer_raw_data, list) and len(layer_raw_data) >= 3:
                    data_len = layer_raw_data[2]
                    
                    # åªæå– s7comm åè®®ä¸­é•¿åº¦ä¸º20çš„æ•°æ®åŒ…
                    if protocol.lower() == 's7comm' and data_len != 20:
                        continue

                    info['data'] = layer_raw_data[0]
                    info['data_len'] = data_len
                    
                    info['split_pos'] = self._extract_split_position(
                        layer_raw_data[1],
                        layer_raw_data[2],
                        layers[protocol]
                    )
                    
                    yield info

    def _extract_mbtcp_from_modbus(self, json_data, raw_data):
        """ä»modbusæ•°æ®ä¸­æå–mbtcpåè®®ä¿¡æ¯ï¼ˆåŒ…å«å®Œæ•´çš„mbtcp+modbusæ•°æ®ï¼‰"""
        for pkt_id, (pkt, pkt_raw) in enumerate(zip(json_data, raw_data)):
            layers = pkt_raw.get('_source', {}).get('layers', {})
            
            # æ£€æŸ¥æ˜¯å¦åŒæ—¶åŒ…å«mbtcpå’Œmodbuså±‚
            if 'mbtcp' in layers and 'mbtcp_raw' in layers and 'modbus' in layers and 'modbus_raw' in layers:
                mbtcp_raw_data = layers['mbtcp_raw']
                
                if isinstance(mbtcp_raw_data, list) and len(mbtcp_raw_data) >= 3:
                    # æå–åŸºæœ¬ä¿¡æ¯
                    info = self._extract_basic_info(pkt)
                    info['pkt_id'] = pkt_id
                    info['protocol'] = 'mbtcp'
                    
                    # ä½¿ç”¨å®Œæ•´çš„mbtcpæ•°æ®ï¼ˆåŒ…å«å†…åµŒçš„modbusï¼‰
                    info['data'] = mbtcp_raw_data[0]
                    info['data_len'] = mbtcp_raw_data[2]
                    
                    # ä¸ºmbtcpåè®®æå–è¯¦ç»†çš„å­—æ®µåˆ†å‰²ï¼ˆåŒ…æ‹¬mbtcpå¤´éƒ¨å­—æ®µå’Œå†…åµŒçš„modbuså­—æ®µï¼‰
                    info['split_pos'] = self._extract_mbtcp_split_position(
                        mbtcp_raw_data[1],
                        mbtcp_raw_data[2],
                        layers
                    )
                    
                    yield info

    def _extract_mbtcp_split_position(self, start_bias: int, data_len: int, layers: Dict) -> Dict:
        """ä¸ºMBTCPåè®®æå–è¯¦ç»†çš„å­—æ®µåˆ†å‰²ä½ç½®ï¼ˆåŒ…æ‹¬mbtcpå¤´å’Œmodbusæ•°æ®ï¼‰"""
        def traverse_field_info(json_node: Union[Dict, List], field_info: Dict, prefix: str = ''):
            """é€’å½’éå†å­—æ®µä¿¡æ¯"""
            if isinstance(json_node, dict):
                for field, field_data in json_node.items():
                    current_prefix = f"{prefix}.{field}" if prefix and not field.endswith(":") else field
                    traverse_field_info(field_data, field_info, prefix=current_prefix)
            elif isinstance(json_node, list):
                field_info[prefix] = json_node

        # æ”¶é›†æ‰€æœ‰å­—æ®µä¿¡æ¯ï¼ˆåŒ…æ‹¬mbtcpå’Œmodbuså±‚ï¼‰
        field_list = {}
        
        # æ·»åŠ mbtcpå±‚å­—æ®µ
        if 'mbtcp' in layers:
            traverse_field_info(layers['mbtcp'], field_list, prefix='mbtcp')
        
        # æ·»åŠ modbuså±‚å­—æ®µ
        if 'modbus' in layers:
            traverse_field_info(layers['modbus'], field_list, prefix='modbus')

        # æå–æ‰€æœ‰å­—æ®µçš„ä½ç½®ä¿¡æ¯
        all_positions = []
        max_bits = data_len * 8

        for field_name, field_info in field_list.items():
            # è·³è¿‡ç©ºå­—æ®µæˆ–æ ¼å¼ä¸æ­£ç¡®çš„å­—æ®µ
            if len(field_info) < 3 or not isinstance(field_info[1], int) or not isinstance(field_info[2], int):
                continue
            
            # è·³è¿‡é•¿åº¦ä¸º0çš„å­—æ®µ
            if field_info[2] == 0:
                continue

            # è¿‡æ»¤ä¸åœ¨å½“å‰åè®®æ•°æ®èŒƒå›´å†…çš„å­—æ®µ
            if field_info[1] < start_bias:
                continue
            
            # è®¡ç®—å­—èŠ‚çº§åˆ«çš„ç›¸å¯¹ä½ç½®
            byte_start = field_info[1] - start_bias
            byte_end = byte_start + field_info[2]
            
            # ç¡®ä¿ä¸è¶…å‡ºæ•°æ®èŒƒå›´
            if byte_start >= data_len:
                continue
            
            # å¦‚æœç»“æŸä½ç½®è¶…å‡ºèŒƒå›´ï¼Œåˆ™æˆªæ–­
            if byte_end > data_len:
                byte_end = data_len

            # è§£æä½æ©ç 
            bit_start, bit_end = self._parse_bitmask(field_info)

            if bit_start >= 0 and bit_end >= 0:
                # ä½å­—æ®µ
                start_pos = byte_start * 8 + bit_start
                end_pos = byte_start * 8 + bit_end
                
                # ç¡®ä¿ä½æ©ç ä¸è¶…å‡ºå­—æ®µèŒƒå›´
                if end_pos > byte_end * 8:
                    end_pos = byte_end * 8
            else:
                # å­—èŠ‚å­—æ®µ
                start_pos = byte_start * 8
                end_pos = byte_end * 8
            
            # æœ€ç»ˆè¾¹ç•Œæ£€æŸ¥
            if start_pos >= max_bits or end_pos > max_bits or start_pos >= end_pos:
                continue
            
            # å­˜å‚¨å­—æ®µä¿¡æ¯ï¼ŒåŒ…æ‹¬åç§°å’Œå±‚çº§æ·±åº¦
            depth = field_name.count('.')
            all_positions.append({
                'start': start_pos,
                'end': end_pos,
                'name': field_name,
                'depth': depth
            })
        
        # æŒ‰å¼€å§‹ä½ç½®å’Œç»“æŸä½ç½®æ’åº
        all_positions.sort(key=lambda p: (p['start'], p['end']))
        
        # æ‰¾å‡ºæœ€ç»†ç²’åº¦çš„å­—æ®µåˆ’åˆ†
        intervals = []
        
        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆå­—æ®µï¼Œè¿”å›ç©ºç»“æœ
        if not all_positions:
            result = {
                'b_pos': [],
                'e_pos': [],
                'detailed_intervals': []
            }
            validation_result = self._validate_split_positions(result, data_len)
            result['validation'] = validation_result
            return result
        
        current_positions = sorted([(p['start'], 'start', i) for i, p in enumerate(all_positions)] + 
                                [(p['end'], 'end', i) for i, p in enumerate(all_positions)])
        
        # ä½¿ç”¨æ‰«æçº¿ç®—æ³•æ‰¾å‡ºæ‰€æœ‰ä¸é‡å çš„åŒºé—´
        open_intervals = set()
        last_pos = None
        
        for pos, event_type, idx in current_positions:
            # ç¡®ä¿ä½ç½®åœ¨æœ‰æ•ˆèŒƒå›´å†…
            if pos > max_bits:
                continue
                
            # å¦‚æœä½ç½®å˜åŒ–ä¸”æœ‰å¼€æ”¾çš„åŒºé—´ï¼Œåˆ™æ·»åŠ ä¸€ä¸ªåŒºé—´
            if last_pos is not None and pos > last_pos and open_intervals:
                # åœ¨å¼€æ”¾åŒºé—´ä¸­æ‰¾å‡ºæ·±åº¦æœ€å¤§çš„ï¼ˆæœ€ç»†ç²’åº¦çš„ï¼‰
                deepest_idx = max(open_intervals, key=lambda i: all_positions[i]['depth'])
                intervals.append((last_pos, pos, deepest_idx))
            
            # æ›´æ–°å¼€æ”¾åŒºé—´é›†åˆ
            if event_type == 'start':
                open_intervals.add(idx)
            else:
                if idx in open_intervals:
                    open_intervals.remove(idx)
            
            last_pos = pos
        
        # ä»åŒºé—´ç”Ÿæˆä¸é‡å çš„ b_pos å’Œ e_pos
        b_pos = []
        e_pos = []
        
        for start, end, _ in intervals:
            if start not in b_pos:
                b_pos.append(start)
            if end not in e_pos:
                e_pos.append(end)
        
        # ç¡®ä¿ b_pos å’Œ e_pos æ˜¯æœ‰åºçš„
        b_pos.sort()
        e_pos.sort()
        
        result = {
            'b_pos': b_pos,
            'e_pos': e_pos,
            'detailed_intervals': [(start, end, all_positions[idx]['name']) for start, end, idx in intervals]
        }

        # æ·»åŠ ä½ç½®éªŒè¯
        validation_result = self._validate_split_positions(result, data_len)
        result['validation'] = validation_result

        return result
       
    def _extract_modbus_with_priority(self, json_data, raw_data, protocol):
        """ä¸ºMODBUSåè®®å®ç°ä¼˜å…ˆçº§ç­›é€‰ï¼šä¼˜å…ˆè¦5å­—èŠ‚é•¿åº¦04å¼€å¤´çš„åŒ…ï¼Œä¸è¶³æ—¶è¡¥å……02å¼€å¤´çš„5å­—èŠ‚åŒ…"""
        # æ”¶é›†æ‰€æœ‰5å­—èŠ‚é•¿åº¦çš„MODBUSåŒ…ï¼ŒæŒ‰åŠŸèƒ½ç åˆ†ç±»
        packets_04_5bytes = []
        packets_02_5bytes = []
        
        for pkt_id, (pkt, pkt_raw) in enumerate(zip(json_data, raw_data)):
            layers = pkt_raw.get('_source', {}).get('layers', {})
            
            if protocol in layers and f"{protocol}_raw" in layers:
                layer_raw_data = layers[f"{protocol}_raw"]
                
                if isinstance(layer_raw_data, list) and len(layer_raw_data) >= 3:
                    data_len = layer_raw_data[2]
                    data = layer_raw_data[0]
                    
                    # åªå¤„ç†5å­—èŠ‚é•¿åº¦çš„åŒ…
                    if data_len != 5:
                        continue
                    
                    # æå–åŸºæœ¬ä¿¡æ¯
                    info = self._extract_basic_info(pkt)
                    info['pkt_id'] = pkt_id
                    info['protocol'] = protocol
                    info['data'] = data
                    info['data_len'] = data_len
                    info['split_pos'] = self._extract_split_position(
                        layer_raw_data[1],
                        layer_raw_data[2],
                        layers[protocol]
                    )
                    
                    # æ ¹æ®åŠŸèƒ½ç åˆ†ç±»ï¼ˆæ•°æ®çš„ç¬¬ä¸€ä¸ªå­—èŠ‚ï¼‰
                    if len(data) >= 2:
                        function_code = data[:2].lower()
                        
                        if function_code == '04':
                            packets_04_5bytes.append(info)
                        elif function_code == '02':
                            packets_02_5bytes.append(info)
        
        print(f"MODBUSåŒ…åˆ†å¸ƒ (5å­—èŠ‚): 04åŠŸèƒ½ç : {len(packets_04_5bytes)} ä¸ª, 02åŠŸèƒ½ç : {len(packets_02_5bytes)} ä¸ª")
        
        # ä¼˜å…ˆè¾“å‡º04åŠŸèƒ½ç çš„åŒ…
        for packet_info in packets_04_5bytes:
            yield packet_info
        
        # å¦‚æœ04åŠŸèƒ½ç çš„åŒ…ä¸è¶³ï¼Œè¡¥å……02åŠŸèƒ½ç çš„åŒ…
        for packet_info in packets_02_5bytes:
            yield packet_info
    
    def extract_all_splits(self) -> Generator[Dict, None, None]:
        """æå–æ‰€æœ‰åè®®çš„æ•°æ®åˆ†å‰²ä¿¡æ¯"""
        protocols_to_process = self.available_protocols
        
        if self.include_protocols:
            protocols_to_process = [p for p in protocols_to_process 
                                  if p.lower() in self.include_protocols]
        
        print(f"Processing protocols: {protocols_to_process}")
        
        for protocol in protocols_to_process:
            if protocol == 'merged':  # è·³è¿‡åˆå¹¶æ–‡ä»¶
                continue
            
            print(f"\nProcessing protocol: {protocol}")
            yield from self.extract_protocol_splits(protocol)
    
    def get_protocol_stats(self) -> Dict[str, int]:
        """è·å–å„åè®®çš„æ•°æ®åŒ…ç»Ÿè®¡"""
        stats = {}
        for protocol in self.available_protocols:
            if protocol == 'merged':
                continue
            try:
                json_data, _ = self.load_protocol_data(protocol)
                stats[protocol] = len(json_data)
            except FileNotFoundError:
                stats[protocol] = 0
        return stats

    def save_protocol_data(self, output_dir: Union[str, Path], max_packets_per_protocol: Optional[int] = None, 
                          save_used_pcaps: bool = True, pcap_source_dir: Union[str, Path] = None) -> Dict[str, int]:
        """
        ä¿å­˜æ‰€æœ‰åè®®çš„æ•°æ®åŒ…ä¿¡æ¯åˆ°JSONæ–‡ä»¶ï¼Œå¹¶å¯é€‰æ‹©ä¿å­˜å®é™…ä½¿ç”¨çš„pcapåŒ…
        
        Args:
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
            max_packets_per_protocol: æ¯ä¸ªåè®®æœ€å¤šä¿å­˜çš„æ•°æ®åŒ…æ•°é‡ï¼ŒNoneè¡¨ç¤ºä¿å­˜æ‰€æœ‰
            save_used_pcaps: æ˜¯å¦ä¿å­˜å®é™…ä½¿ç”¨çš„pcapåŒ…
            pcap_source_dir: åŸå§‹pcapæ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼Œå¦‚æœsave_used_pcapsä¸ºTrueåˆ™å¿…é¡»æä¾›
        
        Returns:
            ä¿å­˜çš„æ•°æ®åŒ…ç»Ÿè®¡ä¿¡æ¯
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºç”¨äºä¿å­˜å®é™…ä½¿ç”¨pcapçš„ç›®å½•
        if save_used_pcaps:
            if pcap_source_dir is None:
                raise ValueError("å¦‚æœè¦ä¿å­˜å®é™…ä½¿ç”¨çš„pcapåŒ…ï¼Œå¿…é¡»æä¾›pcap_source_dirå‚æ•°")
            
            pcap_source_path = Path(pcap_source_dir)
            if not pcap_source_path.exists():
                raise FileNotFoundError(f"PCAPæºç›®å½•ä¸å­˜åœ¨: {pcap_source_path}")
            
            real_pcap_dir = output_path / "real_used_pcaps"
            real_pcap_dir.mkdir(parents=True, exist_ok=True)
        
        # ç”¨äºç»Ÿè®¡ä¿å­˜çš„æ•°æ®åŒ…æ•°é‡
        saved_stats = {}
        
        # å¤„ç†æ¯ä¸ªåè®®
        for protocol in self.available_protocols:
            if protocol == 'merged':  # è·³è¿‡åˆå¹¶æ–‡ä»¶
                continue
            
            if self.include_protocols and protocol.lower() not in self.include_protocols:
                continue
            
            print(f"\nå¤„ç†åè®®: {protocol}")
            
            # æ”¶é›†è¯¥åè®®çš„æ‰€æœ‰æ•°æ®åŒ…
            protocol_packets = []
            packet_count = 0
            used_packet_ids = []  # è®°å½•å®é™…ä½¿ç”¨çš„åŒ…ID
            
            try:
                # æå–åè®®æ•°æ®
                for packet_info in self.extract_protocol_splits(protocol):
                    # å¦‚æœè¾¾åˆ°æœ€å¤§æ•°é‡é™åˆ¶ï¼Œåˆ™åœæ­¢
                    if max_packets_per_protocol and packet_count >= max_packets_per_protocol:
                        break
                    
                    # è®°å½•ä½¿ç”¨çš„åŒ…ID
                    used_packet_ids.append(packet_info['pkt_id'])
                    
                    # æå–åˆ†å‰²ä½ç½®
                    b_pos = packet_info.get('split_pos', {}).get('b_pos', [])
                    e_pos = packet_info.get('split_pos', {}).get('e_pos', [])
                    data = packet_info.get('data', '')
                    
                    # ç”Ÿæˆåˆ†å‰²åçš„æ•°æ®
                    split_data = ""
                    if data and b_pos and e_pos and len(b_pos) == len(e_pos):
                        # ç¡®ä¿ä½ç½®æ˜¯æŒ‰é¡ºåºæ’åˆ—çš„
                        positions = sorted(zip(b_pos, e_pos))
                        
                        # å°†æ•°æ®æŒ‰ä½ç½®åˆ†å‰²
                        for i, (start_bit, end_bit) in enumerate(positions):
                            # è½¬æ¢ä¸ºå­—èŠ‚ä½ç½®
                            start_byte = start_bit // 8
                            end_byte = (end_bit + 7) // 8  # å‘ä¸Šå–æ•´åˆ°å­—èŠ‚è¾¹ç•Œ
                            
                            # ç¡®ä¿ä¸è¶…å‡ºæ•°æ®èŒƒå›´
                            if start_byte >= len(data) // 2:
                                continue
                            if end_byte > len(data) // 2:
                                end_byte = len(data) // 2
                            
                            # æå–è¯¥å­—æ®µçš„æ•°æ®
                            field_data = data[start_byte*2:end_byte*2]
                            
                            # æ·»åŠ åˆ°åˆ†å‰²åçš„æ•°æ®ä¸­ï¼Œç”¨ç©ºæ ¼åˆ†éš”
                            if i > 0:
                                split_data += " "
                            split_data += field_data
                    
                    # æå–å…³é”®ä¿¡æ¯
                    formatted_packet = {
                        'protocol': protocol,
                        'packet_id': packet_info['pkt_id'],
                        'time': packet_info.get('time'),
                        'src_addr': packet_info.get('src', [None, None])[0],
                        'src_port': packet_info.get('src', [None, None])[1],
                        'dst_addr': packet_info.get('dst', [None, None])[0],
                        'dst_port': packet_info.get('dst', [None, None])[1],
                        'stream_id': packet_info.get('stream'),
                        'data': packet_info.get('data'),
                        'data_len': packet_info.get('data_len'),
                        'split_positions': {
                            'b_pos': packet_info.get('split_pos', {}).get('b_pos', []),
                            'e_pos': packet_info.get('split_pos', {}).get('e_pos', []),
                        }
                    }
                    
                    protocol_packets.append(formatted_packet)
                    packet_count += 1
                
                # ä¿å­˜è¯¥åè®®å®é™…ä½¿ç”¨çš„åŒ…IDè®°å½•
                self.used_packet_ids[protocol] = used_packet_ids
                
                # å°†è¯¥åè®®çš„æ•°æ®ä¿å­˜ä¸ºå•ç‹¬çš„JSONæ–‡ä»¶
                if protocol_packets:
                    protocol_output = output_path / f"{protocol}_data.json"
                    with open(protocol_output, 'w', encoding='utf-8') as f:
                        # å…ˆè½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œä½¿ç”¨æ ‡å‡†ç¼©è¿›
                        json_str = json.dumps(protocol_packets, ensure_ascii=False, indent=2)
                        
                        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å°†b_poså’Œe_posæ•°ç»„ä»å¤šè¡Œå˜ä¸ºå•è¡Œï¼Œå¹¶ç§»é™¤å¤šä½™ç©ºæ ¼
                        json_str = re.sub(
                            r'("(?:b_pos|e_pos)":\s*\[)([\d\s,]+)(\])', 
                            lambda m: m.group(1) + ' ' + re.sub(r'\s+', ' ', m.group(2).strip()) + ' ' + m.group(3),
                            json_str
                        )
                        
                        # å†™å…¥æ–‡ä»¶
                        f.write(json_str)
                    
                    saved_stats[protocol] = len(protocol_packets)
                    print(f"å·²ä¿å­˜ {len(protocol_packets)} ä¸ª {protocol} æ•°æ®åŒ…åˆ° {protocol_output}")
                    
                    # ä¿å­˜å®é™…ä½¿ç”¨çš„pcapåŒ…
                    if save_used_pcaps and used_packet_ids:
                        # å¯¹äºmbtcpåè®®ï¼Œä¹Ÿä½¿ç”¨modbusçš„pcapæ–‡ä»¶ï¼Œä½†ä¸å•ç‹¬ç”Ÿæˆmbtcp.pcap
                        source_protocol = 'modbus' if protocol.lower() == 'mbtcp' else protocol
                        self._save_used_pcap_packets(
                            source_protocol, 
                            used_packet_ids, 
                            pcap_source_path, 
                            real_pcap_dir
                        )
                        
                else:
                    print(f"æœªæ‰¾åˆ°ä»»ä½• {protocol} æ•°æ®åŒ…")
                    saved_stats[protocol] = 0
                
            except Exception as e:
                print(f"å¤„ç†åè®® {protocol} æ—¶å‡ºé”™: {e}")
                saved_stats[protocol] = 0
        
        # ä¿å­˜æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯
        stats_output = output_path / "protocol_stats.json"
        with open(stats_output, 'w', encoding='utf-8') as f:
            stats_data = {
                'total_packets': sum(saved_stats.values()),
                'protocols': saved_stats
            }
            
            # å¦‚æœä¿å­˜äº†å®é™…ä½¿ç”¨çš„pcapåŒ…ï¼Œæ·»åŠ ç›¸å…³ä¿¡æ¯
            if save_used_pcaps:
                stats_data['used_packet_ids'] = self.used_packet_ids
                stats_data['real_used_pcaps_dir'] = str(real_pcap_dir)
            
            json.dump(stats_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nå·²ä¿å­˜æ‰€æœ‰åè®®ç»Ÿè®¡ä¿¡æ¯åˆ° {stats_output}")
        print(f"æ€»å…±ä¿å­˜äº† {sum(saved_stats.values())} ä¸ªæ•°æ®åŒ…")
        
        if save_used_pcaps:
            print(f"å®é™…ä½¿ç”¨çš„pcapåŒ…å·²ä¿å­˜åˆ°: {real_pcap_dir}")
        
        return saved_stats
    
    def _save_used_pcap_packets(self, protocol: str, used_packet_ids: List[int], 
                               pcap_source_dir: Path, output_dir: Path):
        """
        ä¿å­˜æŒ‡å®šåè®®å®é™…ä½¿ç”¨çš„pcapåŒ…
        
        Args:
            protocol: åè®®åç§°
            used_packet_ids: å®é™…ä½¿ç”¨çš„åŒ…IDåˆ—è¡¨ï¼ˆä»0å¼€å§‹ï¼‰
            pcap_source_dir: åŸå§‹pcapæ–‡ä»¶ç›®å½•
            output_dir: è¾“å‡ºç›®å½•
        """
        try:
            source_pcap = pcap_source_dir / f"{protocol}.pcap"
            if not source_pcap.exists():
                print(f"âš ï¸  è­¦å‘Š: æºpcapæ–‡ä»¶ä¸å­˜åœ¨: {source_pcap}")
                return
            
            output_pcap = output_dir / f"{protocol}.pcap"
            
            # åˆ›å»ºåŒ…è¿‡æ»¤è¡¨è¾¾å¼ï¼Œtsharkä½¿ç”¨1å¼€å§‹çš„åŒ…ç¼–å·
            # å°†0å¼€å§‹çš„åŒ…IDè½¬æ¢ä¸º1å¼€å§‹çš„åŒ…ç¼–å·
            packet_numbers = [str(pkt_id + 1) for pkt_id in used_packet_ids]
            
            # æ„å»ºtsharkå‘½ä»¤æ¥æå–æŒ‡å®šçš„åŒ…
            # ä½¿ç”¨frame.numberæ¥è¿‡æ»¤ç‰¹å®šçš„åŒ…
            filter_expr = "frame.number==" + " or frame.number==".join(packet_numbers)
            
            command = [
                "tshark", 
                "-r", str(source_pcap),
                "-Y", filter_expr,
                "-w", str(output_pcap)
            ]
            
            print(f"æ­£åœ¨æå– {protocol} åè®®çš„ {len(used_packet_ids)} ä¸ªå®é™…ä½¿ç”¨çš„åŒ…...")
            
            # æ‰§è¡Œå‘½ä»¤
            result = subprocess.run(
                command, 
                capture_output=True, 
                text=True, 
                check=True
            )
            
            # éªŒè¯è¾“å‡ºæ–‡ä»¶
            if output_pcap.exists() and output_pcap.stat().st_size > 0:
                print(f"âœ… æˆåŠŸä¿å­˜ {protocol} å®é™…ä½¿ç”¨çš„pcapåŒ…åˆ°: {output_pcap}")
                
                # éªŒè¯åŒ…æ•°é‡
                verify_command = ["tshark", "-r", str(output_pcap), "-c", "999999"]
                verify_result = subprocess.run(
                    verify_command, 
                    capture_output=True, 
                    text=True
                )
                actual_count = len(verify_result.stdout.strip().split('\n')) if verify_result.stdout.strip() else 0
                print(f"   éªŒè¯: é¢„æœŸ {len(used_packet_ids)} ä¸ªåŒ…ï¼Œå®é™…ä¿å­˜ {actual_count} ä¸ªåŒ…")
                
                if actual_count != len(used_packet_ids):
                    print(f"âš ï¸  è­¦å‘Š: åŒ…æ•°é‡ä¸åŒ¹é…ï¼")
            else:
                print(f"âŒ ä¿å­˜å¤±è´¥: {output_pcap}")
                
        except subprocess.CalledProcessError as e:
            print(f"âŒ æå– {protocol} pcapåŒ…æ—¶å‡ºé”™: {e}")
            print(f"   å‘½ä»¤è¾“å‡º: {e.stdout}")
            print(f"   é”™è¯¯è¾“å‡º: {e.stderr}")
        except Exception as e:
            print(f"âŒ å¤„ç† {protocol} pcapåŒ…æ—¶å‡ºé”™: {e}")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆå§‹åŒ–å¤„ç†å™¨
    handler = ProtocolJsonHandler(
        json_folder="./data/processed/json",
        jsonraw_folder="./data/processed/jsonraw",
        include_protocols=None  # å¤„ç†æ‰€æœ‰åè®®
    )
    
    # print("Available protocols:", handler.available_protocols)
    
    # # è·å–ç»Ÿè®¡ä¿¡æ¯
    # stats = handler.get_protocol_stats()
    # print("Protocol stats:", stats)
    
    # ä¿å­˜æ‰€æœ‰åè®®æ•°æ®åˆ°æŒ‡å®šç›®å½•ï¼Œå¹¶ä¿å­˜å®é™…ä½¿ç”¨çš„pcapåŒ…
    output_dir = "./data/formatted"
    saved_stats = handler.save_protocol_data(
        output_dir=output_dir,
        max_packets_per_protocol=2000,  # æ¯ä¸ªåè®®æœ€å¤šä¿å­˜2000ä¸ªæ•°æ®åŒ…ï¼Œè®¾ä¸ºNoneåˆ™ä¿å­˜æ‰€æœ‰
        save_used_pcaps=True,  # å¯ç”¨ä¿å­˜å®é™…ä½¿ç”¨çš„pcapåŒ…
        pcap_source_dir="./data/protocol"  # åŸå§‹pcapæ–‡ä»¶ç›®å½•
    )
    
    # ç”¨äºè·Ÿè¸ªæ¯ä¸ªåè®®æ˜¯å¦å·²æ‰“å°ç¬¬ä¸€ä¸ªåŒ…
    printed_first = {}
    # æ”¶é›†æœ‰é—®é¢˜çš„æ•°æ®åŒ…
    problem_packets = []
    
    # å¤„ç†æ‰€æœ‰åè®®çš„æ‰€æœ‰æ•°æ®åŒ…
    total_processed = 0
    total_valid = 0
    total_invalid = 0
    
    print("\nå¼€å§‹å¤„ç†æ‰€æœ‰åè®®...")
    
    for split_info in handler.extract_all_splits():
        protocol = split_info['protocol']
        pkt_id = split_info['pkt_id']
        total_processed += 1
        
        # æ£€æŸ¥éªŒè¯ç»“æœ
        validation = split_info.get('split_pos', {}).get('validation', {})
        is_valid = validation.get('is_valid', True)
        has_warnings = bool(validation.get('warnings', []))
        
        if is_valid:
            total_valid += 1
        else:
            total_invalid += 1
            
        # å¦‚æœæ˜¯è¯¥åè®®çš„ç¬¬ä¸€ä¸ªåŒ…ï¼Œæˆ–è€…åŒ…å«é”™è¯¯/è­¦å‘Šï¼Œåˆ™æ”¶é›†ä¿¡æ¯
        if protocol not in printed_first:
            printed_first[protocol] = True
            
            print(f"\n{'='*60}")
            print(f"Protocol: {protocol} - Packet #{pkt_id+1}")
            print(f"{'='*60}")
            
            if validation:
                print(f"ğŸ” VALIDATION RESULTS:")
                print(f"   Valid: {is_valid}")
                if validation.get('errors'):
                    print(f"   âŒ Errors: {validation['errors']}")
                if validation.get('warnings'):
                    print(f"   âš ï¸  Warnings: {validation['warnings']}")
                if validation.get('stats'):
                    stats = validation['stats']
                    print(f"   ğŸ“Š Stats: {stats}")
                print()
            
            # ä½¿ç”¨ json.dumps æ ¼å¼åŒ–è¾“å‡ºå®Œæ•´ä¿¡æ¯
            print(json.dumps(split_info, indent=2, ensure_ascii=False))
        
        # å¦‚æœæ•°æ®åŒ…æœ‰é—®é¢˜ï¼Œæ”¶é›†å®ƒ
        if not is_valid or has_warnings:
            problem_info = {
                'protocol': protocol,
                'pkt_id': pkt_id,
                'is_valid': is_valid,
                'errors': validation.get('errors', []),
                'warnings': validation.get('warnings', []),
                'stats': validation.get('stats', {})
            }
            problem_packets.append(problem_info)
    
    # æ‰“å°æ±‡æ€»ä¿¡æ¯
    print(f"\n{'='*60}")
    print(f"å¤„ç†å®Œæˆ! æ€»ç»“:")
    print(f"{'='*60}")
    print(f"æ€»å¤„ç†æ•°æ®åŒ…: {total_processed}")
    print(f"æœ‰æ•ˆæ•°æ®åŒ…: {total_valid}")
    print(f"æ— æ•ˆæ•°æ®åŒ…: {total_invalid}")
    print(f"æœ‰é—®é¢˜çš„æ•°æ®åŒ…: {len(problem_packets)}")
    
    # æ‰“å°æ‰€æœ‰æœ‰é—®é¢˜çš„æ•°æ®åŒ…
    if problem_packets:
        print(f"\n{'='*60}")
        print(f"æœ‰é—®é¢˜çš„æ•°æ®åŒ…è¯¦æƒ…:")
        print(f"{'='*60}")
        
        for i, problem in enumerate(problem_packets):
            print(f"\né—®é¢˜ #{i+1}:")
            print(f"åè®®: {problem['protocol']}, åŒ…ID: {problem['pkt_id']}")
            print(f"æœ‰æ•ˆæ€§: {'âœ… æœ‰æ•ˆä½†æœ‰è­¦å‘Š' if problem['is_valid'] else 'âŒ æ— æ•ˆ'}")
            
            if problem['errors']:
                print(f"é”™è¯¯: {problem['errors']}")
            if problem['warnings']:
                print(f"è­¦å‘Š: {problem['warnings']}")
            
            # æ‰“å°å…³é”®ç»Ÿè®¡ä¿¡æ¯
            if 'coverage_percentage' in problem['stats']:
                print(f"è¦†ç›–ç‡: {problem['stats'].get('coverage_percentage')}%")
            if 'b_pos_count' in problem['stats'] and 'e_pos_count' in problem['stats']:
                print(f"ä½ç½®æ•°é‡: å¼€å§‹={problem['stats'].get('b_pos_count')}, ç»“æŸ={problem['stats'].get('e_pos_count')}")
